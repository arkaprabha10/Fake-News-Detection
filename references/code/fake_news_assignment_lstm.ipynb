{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "201801076_lab5 (1) (1).ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXC3IFgqevkk",
        "outputId": "6c21d529-db28-4c8e-e3a3-242f8e957166"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "from nltk import word_tokenize\r\n",
        "from string import punctuation\r\n",
        "import torch.optim as optim\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\r\n",
        "import torch.nn.functional as F\r\n",
        "import string\r\n",
        "\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = 'cuda'\r\n",
        "    print('cuda')\r\n",
        "else:\r\n",
        "    device = 'cpu'\r\n",
        "    print('cpu')\r\n",
        "    \r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('stopwords')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uXM_XXseher",
        "outputId": "78b1f125-ca94-479f-8cdd-0981a2b54234"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/fake-news/train.csv')\r\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>Darrell Lucus</td>\n",
              "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
              "      <td>Daniel J. Flynn</td>\n",
              "      <td>Ever get the feeling your life circles the rou...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Why the Truth Might Get You Fired</td>\n",
              "      <td>Consortiumnews.com</td>\n",
              "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
              "      <td>Jessica Purkiss</td>\n",
              "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
              "      <td>Howard Portnoy</td>\n",
              "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ... label\n",
              "0   0  ...     1\n",
              "1   1  ...     0\n",
              "2   2  ...     1\n",
              "3   3  ...     1\n",
              "4   4  ...     1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "y6Q9annxehfK",
        "outputId": "a837050b-dada-4726-c04e-617ab4e1c52e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "df.count()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id        20800\n",
              "title     20242\n",
              "author    18843\n",
              "text      20761\n",
              "label     20800\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1lchFbjehfR",
        "outputId": "fa10194c-f2d1-48fa-a5b2-d3b3cc1a1bc5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "df.dropna(inplace=True)\r\n",
        "df.count()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id        18285\n",
              "title     18285\n",
              "author    18285\n",
              "text      18285\n",
              "label     18285\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbSdOa08ehfW",
        "outputId": "41d374ad-6eec-4068-cd57-e7e4b0bec386"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "print('real news count')\r\n",
        "df[df['label']==0].count()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real news count\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id        10361\n",
              "title     10361\n",
              "author    10361\n",
              "text      10361\n",
              "label     10361\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnmWz91Zehfa",
        "outputId": "391d0c6a-2359-405d-ce11-d7cba5ef8f51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "print('real news count')\r\n",
        "df[df['label']==0].count()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real news count\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id        10361\n",
              "title     10361\n",
              "author    10361\n",
              "text      10361\n",
              "label     10361\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvrRHg3Gehff",
        "outputId": "1c2c9a28-e765-43d5-c3a8-639cbed5ce62"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "stopWords = stopwords.words('english')\r\n",
        "\r\n",
        "def preprocessing(raw_text):\r\n",
        "    texts = [''.join([c for c in text.lower() if c not in punctuation]) for text in raw_text]\r\n",
        "    texts = ''.join(texts)\r\n",
        "    texts = [''.join([c for c in text.lower() if c not in '’']) for text in texts]\r\n",
        "    texts = ''.join(texts)\r\n",
        "    texts = [word for word in word_tokenize(texts) if word not in stopWords]\r\n",
        "    texts = ' '.join(texts)\r\n",
        "    return texts\r\n",
        "\r\n",
        "df['title'] = df['title'].apply(preprocessing)\r\n",
        "df['title'] = df['title'].astype(str)\r\n",
        "df['title']"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        house dem aide didnt even see comeys letter ja...\n",
              "1         flynn hillary clinton big woman campus breitbart\n",
              "2                                    truth might get fired\n",
              "3        15 civilians killed single us airstrike identi...\n",
              "4        iranian woman jailed fictional unpublished sto...\n",
              "                               ...                        \n",
              "20795         rapper ti trump poster child white supremacy\n",
              "20796    nfl playoffs schedule matchups odds new york t...\n",
              "20797    macys said receive takeover approach hudsons b...\n",
              "20798          nato russia hold parallel exercises balkans\n",
              "20799                                      keeps f35 alive\n",
              "Name: title, Length: 18285, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QUwOWvTehfl",
        "outputId": "8169e19d-bd42-45c0-99ae-3397fa27bf18"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "count = [len(df['title'][i]) for i in np.array(df.index)]\r\n",
        "pd.DataFrame(count).describe()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>18285.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>60.250369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>19.168149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>49.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>61.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>72.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>361.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  0\n",
              "count  18285.000000\n",
              "mean      60.250369\n",
              "std       19.168149\n",
              "min        0.000000\n",
              "25%       49.000000\n",
              "50%       61.000000\n",
              "75%       72.000000\n",
              "max      361.000000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "6e9sYkuYehft",
        "outputId": "4e127194-1bb2-40bf-fba5-592c562ee49b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "class FakeNewsDataset(Dataset):\r\n",
        "    def __init__(self):\r\n",
        "        self.titles = df[\"title\"].values\r\n",
        "        self.labels = df[\"label\"].values\r\n",
        "        self.word2id = self.build_vocab()\r\n",
        "        self.max_len = 72\r\n",
        "  \r\n",
        "    def build_vocab(self):\r\n",
        "        # added preprocessing: changing to lowercase and removing punctuations\r\n",
        "        title_tokens = [word_tokenize(title.lower()) for title in self.titles]\r\n",
        "        title_tokens = [token for title_token in title_tokens for token in title_token]\r\n",
        "        # removed duplicate tokens so that length of dictionary and the max index of a word remain the same\r\n",
        "        tokens = list(set([token for token in title_tokens if token.isalpha()]))\r\n",
        "        word2id = {token: i+1 for i, token in enumerate(tokens)}\r\n",
        "        return word2id\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        # load data and labels\r\n",
        "        title = self.titles[idx]\r\n",
        "        label = self.labels[idx]\r\n",
        "        title_vector, label_vector = self.transform(title, label)\r\n",
        "        return title_vector, label_vector\r\n",
        "  \r\n",
        "    def transform(self, title, label):\r\n",
        "        tokens = word_tokenize(title.lower())\r\n",
        "        tokens = [token.lower() for token in tokens if token.isalpha()]\r\n",
        "        title_vector = [self.word2id[token] for token in tokens]\r\n",
        "        if self.max_len > len(title_vector):\r\n",
        "            diff = self.max_len - len(title_vector)\r\n",
        "            pad = [0 for i in range(diff)]\r\n",
        "            title_vector = title_vector + pad\r\n",
        "        if len(title_vector) > self.max_len:\r\n",
        "              title_vector = title_vector[:self.max_len]\r\n",
        "        title_vector = torch.Tensor(title_vector).to(torch.int64)\r\n",
        "        label_vector = torch.Tensor([label]).to(torch.int64)\r\n",
        "        return title_vector, label_vector\r\n",
        "  \r\n",
        "    def __len__(self):\r\n",
        "        return len(self.titles)"
      ],
      "outputs": [],
      "metadata": {
        "id": "qRe_ichUehfz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "fnd = FakeNewsDataset()\r\n",
        "train_data_loader = DataLoader(fnd, shuffle=True, batch_size=512)\r\n",
        "vocab_length = len(fnd.build_vocab())+1\r\n",
        "print(vocab_length)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21555\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvSOzF2aehf9",
        "outputId": "e7569200-c161-45ae-efad-45267f4c2449"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "class PredictFake(torch.nn.Module):\r\n",
        "    def __init__(self, vocab_length, embedding_dim, num_layers, num_hidden):\r\n",
        "        super(PredictFake, self).__init__()\r\n",
        "        self.embedding = nn.Embedding(vocab_length, embedding_dim, max_norm=True)\r\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=num_hidden, num_layers=num_layers, batch_first=True)\r\n",
        "        self.fc = nn.Linear(72*num_hidden, 2)\r\n",
        "        self.sig = nn.Sigmoid()\r\n",
        "    \r\n",
        "    def forward(self, x):\r\n",
        "        # Performing a forward pass of our model\r\n",
        "        batch_size = x.size(0)\r\n",
        "        x = self.embedding(x)\r\n",
        "        x, temp = self.lstm(x)\r\n",
        "        x = torch.reshape(x, (x.size(0),-1,))\r\n",
        "#         x = x.contiguous().view(-1, 256)\r\n",
        "#         x = self.dropout(x)\r\n",
        "#         x = self.fc(x)\r\n",
        "#         x = self.sig(x)\r\n",
        "#         x = x.view(batch_size, -1)\r\n",
        "#         x = x[:, -1] \r\n",
        "\r\n",
        "        return F.log_softmax(x,dim=-1)"
      ],
      "outputs": [],
      "metadata": {
        "id": "hjs7twstehgC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "batch_size = 512\r\n",
        "model = PredictFake(vocab_length, embedding_dim=128, num_layers=1, num_hidden=256)\r\n",
        "# model.to(device)"
      ],
      "outputs": [],
      "metadata": {
        "id": "jwlUVk-9ehgF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "criterion = nn.CrossEntropyLoss()\r\n",
        "optimizer = optim.Adam(params=model.parameters(), lr=0.01)\r\n",
        "\r\n",
        "num_train = round(len(train_data_loader)*0.8)\r\n",
        "num_test = round(len(train_data_loader)*0.2)\r\n",
        "training_data, testing_data = random_split(train_data_loader, [num_train, num_test])\r\n",
        "datasets = {\"Training\":training_data.dataset, \"Validation\":testing_data.dataset}"
      ],
      "outputs": [],
      "metadata": {
        "id": "NnEnc2VhehgJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "n_epochs = 10\r\n",
        "weights_path = '/content/drive/MyDrive/fake-news/weights_path'\r\n",
        "val_losses = [np.inf]\r\n",
        "no_improvement = 0\r\n",
        "\r\n",
        "for epoch in range(n_epochs):\r\n",
        "    for dataset_type in datasets:\r\n",
        "        if dataset_type == \"Training\":\r\n",
        "            model.train(True)\r\n",
        "        else:\r\n",
        "            model.train(False)\r\n",
        "\r\n",
        "        dataset = datasets[dataset_type]\r\n",
        "        total_points = 0\r\n",
        "        run_loss = 0\r\n",
        "        run_accuracy = 0\r\n",
        "        for i, sample in enumerate(dataset):\r\n",
        "            data, labels = sample\r\n",
        "            labels = torch.LongTensor([l for label in labels for l in label])\r\n",
        "#             data, labels = data.to(device), labels.to(device) \r\n",
        "            optimizer.zero_grad()\r\n",
        "            out = model(data)\r\n",
        "            _, pred = torch.max(out, 1)\r\n",
        "            n_correct = (pred == labels).sum()\r\n",
        "            loss = criterion(out,labels)\r\n",
        "\r\n",
        "            if dataset_type == \"Training\":\r\n",
        "                loss.backward()\r\n",
        "                optimizer.step()\r\n",
        "            run_loss += loss.item()\r\n",
        "            run_accuracy  += n_correct.data.item()\r\n",
        "            total_points += len(sample[0])\r\n",
        "\r\n",
        "        print(\"Epoch {}, {} Loss: {}, Accuracy: {}\".format(epoch + 1, dataset_type, run_loss / i, run_accuracy / total_points * 100))\r\n",
        "        if dataset_type == \"Validation\":\r\n",
        "            val_loss = run_loss / i\r\n",
        "            if all(val_losses < np.array(val_loss)):\r\n",
        "#                 torch.save(model.state_dict(), weights_path)\r\n",
        "                no_improvement = 0\r\n",
        "            else:\r\n",
        "                no_improvement += 1\r\n",
        "            val_losses.append(val_loss)\r\n",
        "            if no_improvement == 3:\r\n",
        "                break"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss: 9.377237129211426, Accuracy: 36.292042657916326\n",
            "Epoch 1, Validation Loss: 8.981673104422432, Accuracy: 64.61033634126333\n",
            "Epoch 2, Training Loss: 8.942330850873674, Accuracy: 73.0817610062893\n",
            "Epoch 2, Validation Loss: 8.924029268537248, Accuracy: 81.0117582718075\n",
            "Epoch 3, Training Loss: 8.92157393864223, Accuracy: 81.0664479081214\n",
            "Epoch 3, Validation Loss: 8.919704627990722, Accuracy: 81.57506152584085\n",
            "Epoch 4, Training Loss: 8.918554142543249, Accuracy: 81.33442712605962\n",
            "Epoch 4, Validation Loss: 8.91727019718715, Accuracy: 81.6133442712606\n",
            "Epoch 5, Training Loss: 8.915604591369629, Accuracy: 81.23598578069455\n",
            "Epoch 5, Validation Loss: 8.911989838736398, Accuracy: 81.59693738036641\n",
            "Epoch 6, Training Loss: 8.911101613725934, Accuracy: 81.29614438063987\n",
            "Epoch 6, Validation Loss: 8.910231481279645, Accuracy: 81.59146841673503\n",
            "Epoch 7, Training Loss: 8.909695380074638, Accuracy: 81.23051681706318\n",
            "Epoch 7, Validation Loss: 8.909160723005023, Accuracy: 81.58599945310364\n",
            "Epoch 8, Training Loss: 8.908842550005232, Accuracy: 81.23598578069455\n",
            "Epoch 8, Validation Loss: 8.908512360709054, Accuracy: 81.58599945310364\n",
            "Epoch 9, Training Loss: 8.908309800284249, Accuracy: 81.2742685261143\n",
            "Epoch 9, Validation Loss: 8.908090318952288, Accuracy: 81.58053048947225\n",
            "Epoch 10, Training Loss: 8.907946995326451, Accuracy: 81.37817883511075\n",
            "Epoch 10, Validation Loss: 8.907788249424526, Accuracy: 81.59146841673503\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dGiMhZxehgL",
        "outputId": "3364b52e-1b3d-4411-a2ca-9c65d749fcfc"
      }
    }
  ]
}