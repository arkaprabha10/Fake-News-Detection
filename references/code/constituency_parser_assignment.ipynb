{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "v2kwp69zMVmx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2kwp69zMVmx",
        "outputId": "ad2652f3-e792-43e5-919e-5bbfb0997d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4d246f6b-1f50-495e-bdf5-08c8bb871873",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d246f6b-1f50-495e-bdf5-08c8bb871873",
        "outputId": "db8c4fb6-6e94-426c-b5ed-899cd245e88b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import nltk\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.metrics import Precision, Recall, Accuracy\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Concatenate, TimeDistributed\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "14604806-ada4-4f73-b04c-2a712f3ac0ea",
      "metadata": {
        "id": "14604806-ada4-4f73-b04c-2a712f3ac0ea"
      },
      "outputs": [],
      "source": [
        "def readFile(path):\n",
        "    file = open(path,'r+')\n",
        "    text = file.readlines()\n",
        "    print(len(text))\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "63042fdb-8b5d-4ca2-b8b5-23e6920f2f16",
      "metadata": {
        "id": "63042fdb-8b5d-4ca2-b8b5-23e6920f2f16"
      },
      "outputs": [],
      "source": [
        "def posTagging(text):\n",
        "    tagged_list = []\n",
        "    for txt in text:\n",
        "        tokenized = sent_tokenize(txt)\n",
        "        for i in tokenized:\n",
        "            wordsList = nltk.word_tokenize(i)\n",
        "            wordsList = [w.lower() for w in wordsList] \n",
        "            tagged = nltk.pos_tag(wordsList)\n",
        "            tagged_list.append(tagged)\n",
        "    return tagged_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "317ee42e",
      "metadata": {
        "id": "317ee42e"
      },
      "outputs": [],
      "source": [
        "def getTokensAndLabels(text):\n",
        "    tokens = []\n",
        "    labels = []\n",
        "    for txt in text:\n",
        "        t = []\n",
        "        l = []\n",
        "        for w in txt:\n",
        "            t.append(w[0])\n",
        "            l.append(w[1])\n",
        "#             print(t)\n",
        "        count = t.count('<')\n",
        "        while count>0:\n",
        "            count -= 1\n",
        "            ind = [i for i,x in enumerate(t) if x=='<'][0]\n",
        "#             print(ind)\n",
        "            if t[ind+1]=='unk' and t[ind+2]=='>':\n",
        "                t[ind] = '<unk>'\n",
        "                del t[ind+1]\n",
        "                del t[ind+1]\n",
        "                l[ind] = nltk.pos_tag(['<unk>'])[0][1]\n",
        "                del l[ind+1]\n",
        "                del l[ind+1]\n",
        "        tokens.append(t)\n",
        "        labels.append(l)\n",
        "    return tokens, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "458f1d53",
      "metadata": {
        "id": "458f1d53"
      },
      "outputs": [],
      "source": [
        "def getVocab(text, vocab):\n",
        "    for txt in text:\n",
        "        for w in txt:\n",
        "            vocab.add(w)\n",
        "    return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "78efabcd",
      "metadata": {
        "id": "78efabcd"
      },
      "outputs": [],
      "source": [
        "def charVec(text):\n",
        "    char_arr = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z',\n",
        "                '0','1','2','3','4','5','6','7','8','9',\n",
        "                '-',',',';','.','!','?',':','’','’’','/','\\\\','|','_','@','#','$',\n",
        "                '%','^','&','*','˜','‘','+','-','=','(',')','[',']','{','}', \"'\", '\"']\n",
        "    char_vector = []\n",
        "    for txt in text:\n",
        "#         print(txt)\n",
        "        vec = []\n",
        "        for word in txt:\n",
        "#             print(word)\n",
        "            if word == '<unk>':\n",
        "                vec.append([len(char_arr)])\n",
        "            else:\n",
        "                v = []\n",
        "                for c in word:\n",
        "                    v.append(char_arr.index(c))\n",
        "                vec.append(v)\n",
        "        char_vector.append(vec)\n",
        "    return char_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "35b7c751",
      "metadata": {
        "id": "35b7c751"
      },
      "outputs": [],
      "source": [
        "def postagVec(postags, labels):\n",
        "    pos_dict = {}\n",
        "    ind = 0\n",
        "    for tag in postags:\n",
        "        pos_dict[tag] = ind\n",
        "        ind += 1\n",
        "    postag_vector = []\n",
        "    for l in labels:\n",
        "        pos_vec = []\n",
        "        for tag in l:\n",
        "            pos_vec.append(pos_dict[tag])\n",
        "        postag_vector.append(pos_vec)\n",
        "    return postag_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a009fb33",
      "metadata": {
        "id": "a009fb33"
      },
      "outputs": [],
      "source": [
        "def wordVec(text, vocab):\n",
        "    word_dict = {}\n",
        "    ind = 0\n",
        "    for word in vocab:\n",
        "        word_dict[word] = ind\n",
        "        ind += 1\n",
        "    word_vector = []\n",
        "    for txt in text:\n",
        "        w_vec = []\n",
        "        for word in txt:\n",
        "            w_vec.append(word_dict[word])\n",
        "        word_vector.append(w_vec)\n",
        "    return word_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f35672f4",
      "metadata": {
        "id": "f35672f4"
      },
      "outputs": [],
      "source": [
        "def padding(seq, maxlen=150):\n",
        "    final = []\n",
        "    for lis in seq:\n",
        "#         print(lis, '\\n***\\n')\n",
        "        # padding\n",
        "        if len(lis)<maxlen:\n",
        "            pad = []\n",
        "            for i in range(maxlen-len(lis)):\n",
        "                if type(lis[0]) == int:\n",
        "                    pad.append(0)\n",
        "                else:\n",
        "                    pad.append([0 for i in range(len(lis[0]))])\n",
        "            for i in range(len(lis)):\n",
        "                pad.append(lis[i])\n",
        "            final.append(pad)\n",
        "        #truncating\n",
        "        else:\n",
        "            trunc = []\n",
        "            for i in range(maxlen):\n",
        "                trunc.append(lis[i])\n",
        "            final.append(trunc)\n",
        "    return final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "0f0745cd-a054-4985-8cdb-e9e04005a33c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f0745cd-a054-4985-8cdb-e9e04005a33c",
        "outputId": "20543c56-3c87-4ff3-ed98-6fc385cd0dd6",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42068\n",
            "3761\n",
            "3370\n"
          ]
        }
      ],
      "source": [
        "## loading files\n",
        "\n",
        "train = readFile('/content/drive/MyDrive/NLP_lab7/ptbdataset/ptb.train.txt')\n",
        "test = readFile('/content/drive/MyDrive/NLP_lab7/ptbdataset/ptb.test.txt')\n",
        "valid = readFile('/content/drive/MyDrive/NLP_lab7/ptbdataset/ptb.valid.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d0d15d0c-d1bb-4bd7-a6aa-e1f5534a96ce",
      "metadata": {
        "id": "d0d15d0c-d1bb-4bd7-a6aa-e1f5534a96ce",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "## pos tagging\n",
        "\n",
        "train_tagged = posTagging(train)\n",
        "test_tagged = posTagging(test)\n",
        "valid_tagged = posTagging(valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b8f0144a",
      "metadata": {
        "id": "b8f0144a"
      },
      "outputs": [],
      "source": [
        "train_tokens, train_postags = getTokensAndLabels(train_tagged)\n",
        "test_tokens, test_postags = getTokensAndLabels(test_tagged)\n",
        "valid_tokens, valid_postags = getTokensAndLabels(valid_tagged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "af076565",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af076565",
        "outputId": "e0796a97-05b8-488a-ed40-8e2a2b9b45e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10004"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## get vocab\n",
        "\n",
        "vocab = set()\n",
        "vocab = getVocab(train_tokens, vocab)\n",
        "vocab = getVocab(test_tokens, vocab)\n",
        "vocab = getVocab(valid_tokens, vocab)\n",
        "vocab = list(vocab)\n",
        "vocab.sort()\n",
        "len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3b968aaa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b968aaa",
        "outputId": "ab20d348-37fb-48e2-faec-e1b6fd50a689"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## pos tags\n",
        "\n",
        "postags = set()\n",
        "postags = getVocab(train_postags, postags)\n",
        "postags = list(postags)\n",
        "postags.sort()\n",
        "len(postags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b4eb097f",
      "metadata": {
        "id": "b4eb097f"
      },
      "outputs": [],
      "source": [
        "# pos tags\n",
        "\n",
        "## convert to vectors\n",
        "\n",
        "pos_seq_train = postagVec(postags, train_postags)\n",
        "pos_seq_test = postagVec(postags, test_postags)\n",
        "pos_seq_valid = postagVec(postags, valid_postags)\n",
        "\n",
        "## padding\n",
        "\n",
        "pos_seq_train = np.array(padding(pos_seq_train), dtype='float32')\n",
        "pos_seq_test = np.array(padding(pos_seq_test), dtype='float32')\n",
        "pos_seq_valid = np.array(padding(pos_seq_valid), dtype='float32')\n",
        "\n",
        "## saving as csv\n",
        "\n",
        "pd.DataFrame(pos_seq_train).to_csv('/content/drive/MyDrive/NLP_lab7/seq_data/pos_seq_train.csv', index=False)\n",
        "pd.DataFrame(pos_seq_test).to_csv('/content/drive/MyDrive/NLP_lab7/seq_data/pos_seq_test.csv', index=False)\n",
        "pd.DataFrame(pos_seq_valid).to_csv('/content/drive/MyDrive/NLP_lab7/seq_data/pos_seq_valid.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8a1430f2",
      "metadata": {
        "id": "8a1430f2"
      },
      "outputs": [],
      "source": [
        "# words\n",
        "\n",
        "## convert to vectors \n",
        "\n",
        "word_seq_train = wordVec(train_tokens, vocab)\n",
        "word_seq_test = wordVec(test_tokens, vocab)\n",
        "word_seq_valid = wordVec(valid_tokens, vocab)\n",
        "\n",
        "## padding\n",
        "\n",
        "word_seq_train = np.array(padding(word_seq_train), dtype='float32')\n",
        "word_seq_test = np.array(padding(word_seq_test), dtype='float32')\n",
        "word_seq_valid = np.array(padding(word_seq_valid), dtype='float32')\n",
        "\n",
        "## saving as csv\n",
        "\n",
        "pd.DataFrame(pos_seq_train).to_csv('/content/drive/MyDrive/NLP_lab7/seq_data/word_seq_train.csv', index=False)\n",
        "pd.DataFrame(pos_seq_test).to_csv('/content/drive/MyDrive/NLP_lab7/seq_data/word_seq_test.csv', index=False)\n",
        "pd.DataFrame(pos_seq_valid).to_csv('/content/drive/MyDrive/NLP_lab7/seq_data/word_seq_valid.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ac923239",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac923239",
        "outputId": "85ed41ef-fb20-489b-a9ac-eeb095a91ca3",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(42503, 150, 40)\n",
            "(3793, 150, 40)\n",
            "(3395, 150, 40)\n"
          ]
        }
      ],
      "source": [
        "# characters \n",
        "\n",
        "## convert to vectors\n",
        "\n",
        "char_seq_train = charVec(train_tokens)\n",
        "char_seq_test = charVec(test_tokens)\n",
        "char_seq_valid = charVec(valid_tokens)\n",
        "\n",
        "## padding \n",
        "\n",
        "for i in range(len(char_seq_train)):\n",
        "    char_seq_train[i] = padding(char_seq_train[i], maxlen=40)\n",
        "char_seq_train = np.array(padding(char_seq_train), dtype='float32')\n",
        "\n",
        "for i in range(len(char_seq_test)):\n",
        "    char_seq_test[i] = padding(char_seq_test[i], maxlen=40)\n",
        "char_seq_test = np.array(padding(char_seq_test), dtype='float32')\n",
        "\n",
        "for i in range(len(char_seq_valid)):\n",
        "    char_seq_valid[i] = padding(char_seq_valid[i], maxlen=40)\n",
        "char_seq_valid = np.array(padding(char_seq_valid), dtype='float32')\n",
        "\n",
        "## saving as csv\n",
        "\n",
        "pd.DataFrame(np.ravel(char_seq_train)).to_csv('/content/drive/MyDrive/NLP_lab7/seq_data/char_seq_train.csv', index=False)\n",
        "pd.DataFrame(np.ravel(char_seq_test)).to_csv('/content/drive/MyDrive/NLP_lab7/seq_data/char_seq_test.csv', index=False)\n",
        "pd.DataFrame(np.ravel(char_seq_valid)).to_csv('/content/drive/MyDrive/NLP_lab7/seq_data/char_seq_valid.csv', index=False)\n",
        "\n",
        "## dimensions to reshape when loading \n",
        "print(char_seq_train.shape)\n",
        "print(char_seq_test.shape)\n",
        "print(char_seq_valid.shape)\n",
        "\n",
        "## saving this to text file\n",
        "dim = np.array(['char train shape = (42503, 150, 40)', \n",
        "                'char test shape = (3793, 150, 40)', \n",
        "                'char valid shape = (3395, 150, 40)', \n",
        "                'reshape command example = np.reshape(np.ravel(char_seq_train), (42503, 150, 40)).shape'])\n",
        "np.savetxt('char_dim.txt', dim, delimiter=',', fmt='%s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "4225c232",
      "metadata": {
        "id": "4225c232"
      },
      "outputs": [],
      "source": [
        "def getPosCat(pos_tag_seq):\n",
        "    pos_tag_seq_cat = []\n",
        "    n = 40\n",
        "    for pos_tag in pos_tag_seq:\n",
        "        pos_tag_seq_cat.append(to_categorical(pos_tag, num_classes = n+1))\n",
        "    return np.array(pos_tag_seq_cat, dtype = 'float32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "61cd403f",
      "metadata": {
        "id": "61cd403f"
      },
      "outputs": [],
      "source": [
        "X_train_char = char_seq_train\n",
        "X_train_word = word_seq_train\n",
        "y_train = getPosCat(pos_seq_train)\n",
        "\n",
        "X_test_char = char_seq_test\n",
        "X_test_word = word_seq_test\n",
        "y_test = getPosCat(pos_seq_test)\n",
        "\n",
        "X_valid_char = char_seq_valid\n",
        "X_valid_word = word_seq_valid\n",
        "y_valid = getPosCat(pos_seq_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "065abbea",
      "metadata": {
        "id": "065abbea"
      },
      "outputs": [],
      "source": [
        "def ConstituencyParser(word_seq_len, char_seq_len, lstm_units, embedding_dim, char_vocab_size, word_vocab_size, pos_vocab_size):\n",
        "    char_size_seq = Input(shape=(word_seq_len, char_seq_len))\n",
        "    word_size_seq = Input(shape=(word_seq_len,))\n",
        "    char_vector = Embedding(char_vocab_size, embedding_dim, input_length=char_seq_len)(char_size_seq)\n",
        "    char_lstm = TimeDistributed(LSTM(lstm_units))(char_vector)\n",
        "    word_vector = Embedding(word_vocab_size, embedding_dim, input_length=word_seq_len)(word_size_seq)\n",
        "    concat_layer = Concatenate(axis=-1)([word_vector, char_lstm])\n",
        "    bilstm = Bidirectional(LSTM(lstm_units, return_sequences=True))(concat_layer)\n",
        "    mlp_1 = Dense(lstm_units, activation=\"relu\")(bilstm)\n",
        "    mlp_2 = Dense(pos_vocab_size+1)(mlp_1)\n",
        "    output = Dense(pos_vocab_size+1, activation=\"softmax\")(mlp_2)\n",
        "    \n",
        "    return Model(inputs=[word_size_seq, char_size_seq], outputs=output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1358a37c",
      "metadata": {
        "id": "1358a37c"
      },
      "outputs": [],
      "source": [
        "char_vocab_size = len(np.unique(char_seq_train)) + 1\n",
        "word_vocab_size = len(vocab) + 1\n",
        "pos_vocab_size = len(postags) + 1\n",
        "embedding_dim = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af707a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5af707a9",
        "outputId": "d246216b-9036-44cd-c1c5-20e55d8975fb",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(42503, 150, 40)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "char_seq_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f82baa3c",
      "metadata": {
        "id": "f82baa3c"
      },
      "outputs": [],
      "source": [
        "s_maxlen = char_seq_train.shape[1]\n",
        "w_maxlen = char_seq_train.shape[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e1c3ec9e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1c3ec9e",
        "outputId": "5808a448-10bc-4af5-a724-c7cf30809582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 150, 40)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 150, 40, 256) 12032       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 150, 256)     2561280     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed (TimeDistribut (None, 150, 128)     197120      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 150, 384)     0           embedding_1[0][0]                \n",
            "                                                                 time_distributed[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, 150, 256)     525312      concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 150, 128)     32896       bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 150, 41)      5289        dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 150, 41)      1722        dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 3,335,651\n",
            "Trainable params: 3,335,651\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = ConstituencyParser(s_maxlen, w_maxlen, 128, embedding_dim, char_vocab_size, word_vocab_size, pos_vocab_size)\n",
        "model.compile(optimizer=Adam(), loss=\"categorical_crossentropy\", metrics=[Accuracy(), Precision(), Recall()])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "4fa22cdc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fa22cdc",
        "outputId": "d79600f5-4346-4a37-f240-741c3a0cf7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "333/333 [==============================] - 285s 825ms/step - loss: 0.2416 - accuracy: 0.0012 - precision: 0.9917 - recall: 0.9168 - val_loss: 0.0330 - val_accuracy: 0.0159 - val_precision: 0.9933 - val_recall: 0.9884\n",
            "Epoch 2/10\n",
            "333/333 [==============================] - 275s 826ms/step - loss: 0.0243 - accuracy: 0.0187 - precision: 0.9948 - recall: 0.9913 - val_loss: 0.0216 - val_accuracy: 0.0198 - val_precision: 0.9948 - val_recall: 0.9923\n",
            "Epoch 3/10\n",
            "333/333 [==============================] - 274s 823ms/step - loss: 0.0176 - accuracy: 0.0196 - precision: 0.9957 - recall: 0.9934 - val_loss: 0.0191 - val_accuracy: 0.0200 - val_precision: 0.9953 - val_recall: 0.9930\n",
            "Epoch 4/10\n",
            "333/333 [==============================] - 274s 822ms/step - loss: 0.0143 - accuracy: 0.0198 - precision: 0.9963 - recall: 0.9946 - val_loss: 0.0180 - val_accuracy: 0.0200 - val_precision: 0.9955 - val_recall: 0.9936\n",
            "Epoch 5/10\n",
            "333/333 [==============================] - 276s 827ms/step - loss: 0.0123 - accuracy: 0.0196 - precision: 0.9967 - recall: 0.9953 - val_loss: 0.0182 - val_accuracy: 0.0194 - val_precision: 0.9955 - val_recall: 0.9938\n",
            "Epoch 6/10\n",
            "333/333 [==============================] - 273s 821ms/step - loss: 0.0106 - accuracy: 0.0196 - precision: 0.9971 - recall: 0.9960 - val_loss: 0.0178 - val_accuracy: 0.0199 - val_precision: 0.9955 - val_recall: 0.9942\n",
            "Epoch 7/10\n",
            "333/333 [==============================] - 275s 827ms/step - loss: 0.0091 - accuracy: 0.0197 - precision: 0.9975 - recall: 0.9966 - val_loss: 0.0179 - val_accuracy: 0.0198 - val_precision: 0.9954 - val_recall: 0.9943\n",
            "Epoch 8/10\n",
            "333/333 [==============================] - 278s 836ms/step - loss: 0.0078 - accuracy: 0.0198 - precision: 0.9978 - recall: 0.9970 - val_loss: 0.0186 - val_accuracy: 0.0201 - val_precision: 0.9954 - val_recall: 0.9944\n",
            "Epoch 9/10\n",
            "333/333 [==============================] - 279s 838ms/step - loss: 0.0067 - accuracy: 0.0200 - precision: 0.9981 - recall: 0.9975 - val_loss: 0.0193 - val_accuracy: 0.0202 - val_precision: 0.9954 - val_recall: 0.9946\n",
            "Epoch 10/10\n",
            "333/333 [==============================] - 279s 838ms/step - loss: 0.0057 - accuracy: 0.0201 - precision: 0.9984 - recall: 0.9979 - val_loss: 0.0201 - val_accuracy: 0.0202 - val_precision: 0.9952 - val_recall: 0.9945\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    [X_train_word, X_train_char], \n",
        "    y_train, \n",
        "    batch_size=128, \n",
        "    epochs=10,\n",
        "    validation_data=([X_valid_word, X_valid_char], y_valid)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "IRH6p0UiN833",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRH6p0UiN833",
        "outputId": "912934c9-ab5f-4883-f994-f9bdd8215302"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "119/119 [==============================] - 9s 78ms/step - loss: 0.0177 - accuracy: 0.0202 - precision: 0.9957 - recall: 0.9951\n"
          ]
        }
      ],
      "source": [
        "scores = model.evaluate([X_test_word, X_test_char],y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "YnszwPlim3ka",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnszwPlim3ka",
        "outputId": "abb392b6-605e-4b5a-aa3b-598da36bdf3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.995675265789032, Recall: 0.9950505495071411, F1 score: 0.9953628096259406\n"
          ]
        }
      ],
      "source": [
        "f1_score = 2* (scores[2]*scores[3])/(scores[2] + scores[3])\n",
        "print(\"Precision: {}, Recall: {}, F1 score: {}\".format(scores[2],scores[3],f1_score))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "201801076_IT412_Assignment7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
