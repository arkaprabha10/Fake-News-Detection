{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Build an RNN to identify unreliable news articles\n",
    "\n",
    "This notebook applies an RNN to identify when an article might be fake news. The data were obtained from the [Fake News Competition on Kaggle](https://www.kaggle.com/c/fake-news). \n",
    "\n",
    ">Using an RNN rather than a strictly feedforward network is more accurate since we can include information about the *sequence* of words. \n",
    "\n",
    "**Performance on real test set from Kaggle:** My Kaggle submission resulted in **92.09%** for private score and  **91.41%** for public score.\n",
    "\n",
    "**Credit:** This is a side project for [PyTorch Scholarship Challenge from Facebook](https://www.udacity.com/facebook-pytorch-scholarship), which uses the [Sentiment_RNN](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb) template from the program. \n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "The architecture for this network is shown below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/minhkhang1795/FakeNews_RNN/master/assets/network_diagram.png\" width=40%>\n",
    "\n",
    ">**First, we'll pass in words to an embedding layer.** We need an embedding layer because we have tens of thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. You should have seen this before from the Word2Vec lesson. You can actually train an embedding with the Skip-gram Word2Vec model and use those embeddings as input, here. However, it's good enough to just have an embedding layer and let the network learn a different embedding table on its own. *In this case, the embedding layer is for dimensionality reduction, rather than for learning semantic representations.*\n",
    "\n",
    ">**After input words are passed to an embedding layer, the new embeddings will be passed to LSTM cells.** The LSTM cells will add *recurrent* connections to the network and give us the ability to include information about the *sequence* of words in the article data. \n",
    "\n",
    ">**Finally, the LSTM outputs will go to a sigmoid output layer.** We're using a sigmoid function because positive (or fake news) = 1 and negative = 0, and a sigmoid will output predicted, sentiment values between 0-1. \n",
    "\n",
    "We don't care about the sigmoid outputs except for the **very last one**; we can ignore the rest. We'll calculate the loss by comparing the output at the last time step and the training label (pos or neg).\n",
    "\n",
    "## Load and visualize the data\n",
    "**train.csv**: A full training dataset with the following attributes:\n",
    "\n",
    "- **id**: unique id for a news article\n",
    "- **title**: the title of a news article\n",
    "- **author**: author of the news article\n",
    "- **text**: the text of the article; could be incomplete\n",
    "- **label**: a label that marks the article as potentially unreliable\n",
    "    - 1: unreliable\n",
    "    - 0: reliable\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_fpath = 'train.csv'\n",
    "train_raw_data = pd.read_csv(train_fpath, encoding='utf-8', header=0, keep_default_na=False)\n",
    "train_raw_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Jackie Mason: Hollywood Would Love Trump if He...</td>\n",
       "      <td>Daniel Nussbaum</td>\n",
       "      <td>In these trying times, Jackie Mason is the Voi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Life: Life Of Luxury: Elton John’s 6 Favorite ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>Ever wonder how Britain’s most iconic pop pian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Benoît Hamon Wins French Socialist Party’s Pre...</td>\n",
       "      <td>Alissa J. Rubin</td>\n",
       "      <td>PARIS  —   France chose an idealistic, traditi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Excerpts From a Draft Script for Donald Trump’...</td>\n",
       "      <td>nan</td>\n",
       "      <td>Donald J. Trump is scheduled to make a highly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>A Back-Channel Plan for Ukraine and Russia, Co...</td>\n",
       "      <td>Megan Twohey and Scott Shane</td>\n",
       "      <td>A week before Michael T. Flynn resigned as nat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Obama’s Organizing for Action Partners with So...</td>\n",
       "      <td>Aaron Klein</td>\n",
       "      <td>Organizing for Action, the activist group that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>BBC Comedy Sketch \"Real Housewives of ISIS\" Ca...</td>\n",
       "      <td>Chris Tomlinson</td>\n",
       "      <td>The BBC produced spoof on the “Real Housewives...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Russian Researchers Discover Secret Nazi Milit...</td>\n",
       "      <td>Amando Flavio</td>\n",
       "      <td>The mystery surrounding The Third Reich and Na...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>US Officials See No Link Between Trump and Russia</td>\n",
       "      <td>Jason Ditz</td>\n",
       "      <td>Clinton Campaign Demands FBI Affirm Trump's Ru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Re: Yes, There Are Paid Government Trolls On S...</td>\n",
       "      <td>AnotherAnnie</td>\n",
       "      <td>Yes, There Are Paid Government Trolls On Socia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>In Major League Soccer, Argentines Find a Home...</td>\n",
       "      <td>Jack Williams</td>\n",
       "      <td>Guillermo Barros Schelotto was not the first A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Wells Fargo Chief Abruptly Steps Down - The Ne...</td>\n",
       "      <td>Michael Corkery and Stacy Cowley</td>\n",
       "      <td>The scandal engulfing Wells Fargo toppled its ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Anonymous Donor Pays $2.5 Million To Release E...</td>\n",
       "      <td>Starkman</td>\n",
       "      <td>A Caddo Nation tribal leader has just been fre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>FBI Closes In On Hillary!</td>\n",
       "      <td>The Doc</td>\n",
       "      <td>FBI Closes In On Hillary! Posted on Home » Hea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Chuck Todd: ’BuzzFeed Did Donald Trump a Polit...</td>\n",
       "      <td>Jeff Poor</td>\n",
       "      <td>Wednesday after   Donald Trump’s press confere...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>News: Hope For The GOP: A Nude Paul Ryan Has J...</td>\n",
       "      <td>nan</td>\n",
       "      <td>Email \\nSince Donald Trump entered the electio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Monica Lewinsky, Clinton Sex Scandal Set for ’...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Screenwriter Ryan Murphy, who has produced the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Rob Reiner: Trump Is ’Mentally Unstable’ - Bre...</td>\n",
       "      <td>Pam Key</td>\n",
       "      <td>Sunday on MSNBC’s “AM Joy,” actor and director...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Massachusetts Cop’s Wife Busted for Pinning Fa...</td>\n",
       "      <td>nan</td>\n",
       "      <td>Massachusetts Cop’s Wife Busted for Pinning Fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Abortion Pill Orders Rise in 7 Latin American ...</td>\n",
       "      <td>Donald G. McNeil Jr. and Pam Belluck</td>\n",
       "      <td>Orders for abortion pills by women in seven La...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Nukes and the UN: a Historic Treaty to Ban Nuc...</td>\n",
       "      <td>Ira Helfand</td>\n",
       "      <td>Email \\nIn an historic move the United Nations...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>EXCLUSIVE: Islamic State Supporters Vow to ‘Sh...</td>\n",
       "      <td>Aaron Klein and Ali Waked</td>\n",
       "      <td>JERUSALEM  —   Islamic State sympathizers and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Humiliated Hillary Tries To Hide What Camera C...</td>\n",
       "      <td>Amanda Shea</td>\n",
       "      <td>Humiliated Hillary Tries To Hide What Camera C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Andrea Tantaros of Fox News Claims Retaliation...</td>\n",
       "      <td>Jim Dwyer</td>\n",
       "      <td>Andrea Tantaros, a former Fox News host, charg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>How Hillary Clinton Became a Hawk - The New Yo...</td>\n",
       "      <td>Mark Landler</td>\n",
       "      <td>Hillary Clinton sat in the hideaway study off ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20770</th>\n",
       "      <td>20770</td>\n",
       "      <td>HUMA ABEDIN SWORE UNDER OATH SHE GAVE UP ‘ALL ...</td>\n",
       "      <td>Iron Sheik</td>\n",
       "      <td>Home › POLITICS | US NEWS › HUMA ABEDIN SWORE ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20771</th>\n",
       "      <td>20771</td>\n",
       "      <td></td>\n",
       "      <td>Letsbereal</td>\n",
       "      <td>DYN's Statement on Last Week's Botnet Attack h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20772</th>\n",
       "      <td>20772</td>\n",
       "      <td></td>\n",
       "      <td>beersession</td>\n",
       "      <td>Kinda reminds me of when Carter gave away the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20773</th>\n",
       "      <td>20773</td>\n",
       "      <td>Australia to hunt down anti-vax nurses and pro...</td>\n",
       "      <td>Vicki Batts</td>\n",
       "      <td>Australia to hunt down anti-vax nurses and pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20774</th>\n",
       "      <td>20774</td>\n",
       "      <td>Government Report: Islamists Building ’Paralle...</td>\n",
       "      <td>Liam Deacon</td>\n",
       "      <td>Aided by a politically correct culture of “tol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20775</th>\n",
       "      <td>20775</td>\n",
       "      <td>How this WWII airman is helping veterans heal ...</td>\n",
       "      <td>Arnaldo Rodgers</td>\n",
       "      <td>‹ › Arnaldo Rodgers is a trained and educated ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20776</th>\n",
       "      <td>20776</td>\n",
       "      <td>Trump Campaign Says Hillary Supporter Tried As...</td>\n",
       "      <td>Jameson Parker</td>\n",
       "      <td>Donald Trump was rushed from a rally stage by ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20777</th>\n",
       "      <td>20777</td>\n",
       "      <td>Editor of Austria’s Largest Paper Charged with...</td>\n",
       "      <td>admin</td>\n",
       "      <td>Breitbart October 26, 2016 \\nAn editor of Aust...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20778</th>\n",
       "      <td>20778</td>\n",
       "      <td>This Is a Jobs Report That Democrats Can Boast...</td>\n",
       "      <td>Neil Irwin</td>\n",
       "      <td>There’s not much to say about the July jobs nu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20779</th>\n",
       "      <td>20779</td>\n",
       "      <td>Christians in 2017 ’Most Persecuted Group in t...</td>\n",
       "      <td>Thomas D. Williams, Ph.D.</td>\n",
       "      <td>In many parts of the world, Christians gatheri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20780</th>\n",
       "      <td>20780</td>\n",
       "      <td>Florida Woman Charged in Death of Infant in ‘C...</td>\n",
       "      <td>Christine Hauser</td>\n",
       "      <td>Early on Oct. 6, Erin   was awakened by the so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20781</th>\n",
       "      <td>20781</td>\n",
       "      <td>Time is Running Out to Stop Kratom Ban – Need ...</td>\n",
       "      <td>Heather Callaghan</td>\n",
       "      <td>By Brandon Turbeville When the DEA announced t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20782</th>\n",
       "      <td>20782</td>\n",
       "      <td>The Fix Is In: NBC Affiliate Accidentally Post...</td>\n",
       "      <td>The Doc</td>\n",
       "      <td>Home » Headlines » World News » The Fix Is In:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20783</th>\n",
       "      <td>20783</td>\n",
       "      <td>Samsung, Kim Jong-un, Rex Tillerson: Your Morn...</td>\n",
       "      <td>Charles McDermid</td>\n",
       "      <td>Good morning.  Here’s what you need to know: •...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20784</th>\n",
       "      <td>20784</td>\n",
       "      <td>Comment on World Heaves Sigh of Relief after T...</td>\n",
       "      <td>Debbie Menon</td>\n",
       "      <td>Finian Cunningham has written extensively on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20785</th>\n",
       "      <td>20785</td>\n",
       "      <td>Ann Coulter: How to Provide Universal Health C...</td>\n",
       "      <td>Ann Coulter</td>\n",
       "      <td>The first sentence of Congress’ Obamacare repe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20786</th>\n",
       "      <td>20786</td>\n",
       "      <td>Government Forces Advancing at Damascus-Aleppo...</td>\n",
       "      <td>nan</td>\n",
       "      <td>#FROMTHEFRONT #MAPS 22.11.2016 - 1,361 views 5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20787</th>\n",
       "      <td>20787</td>\n",
       "      <td>Sally Yates Won’t Say If Trump Was Wiretapped ...</td>\n",
       "      <td>Ian Mason</td>\n",
       "      <td>Former Deputy Attorney General Sally Yates dec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20788</th>\n",
       "      <td>20788</td>\n",
       "      <td>Maine’s Gov. LePage Threatens To ‘Investigate’...</td>\n",
       "      <td>Joe Clark</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20789</th>\n",
       "      <td>20789</td>\n",
       "      <td>Sen. McConnell: The Supreme Court Vacancy Was ...</td>\n",
       "      <td>Warner Todd Huston</td>\n",
       "      <td>Senate Majority Leader Mitch McConnell (R, KY)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20790</th>\n",
       "      <td>20790</td>\n",
       "      <td>Nikki Haley Blasts U.N. Human Rights Office fo...</td>\n",
       "      <td>Adam Shaw</td>\n",
       "      <td>U. S Ambassador to the United Nations Nikki Ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20791</th>\n",
       "      <td>20791</td>\n",
       "      <td>Lawyer Who Kept Hillary Campaign Chief Out of ...</td>\n",
       "      <td>Daniel Greenfield</td>\n",
       "      <td>Lawyer Who Kept Hillary Campaign Chief Out of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20792</th>\n",
       "      <td>20792</td>\n",
       "      <td>Jakarta Bombing Kills Three Police Officers, L...</td>\n",
       "      <td>John Hayward</td>\n",
       "      <td>Two suicide bombers attacked a bus station in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20793</th>\n",
       "      <td>20793</td>\n",
       "      <td>Idiot Who Destroyed Trump Hollywood Star Gets ...</td>\n",
       "      <td>Robert Rich</td>\n",
       "      <td>Share This \\nAlthough the vandal who thought i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20794</th>\n",
       "      <td>20794</td>\n",
       "      <td>Trump: Putin ’Very Smart’ to Not Retaliate ove...</td>\n",
       "      <td>Lee Stranahan</td>\n",
       "      <td>Donald Trump took to Twitter Friday to praise ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20796</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20799</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "5          5  Jackie Mason: Hollywood Would Love Trump if He...   \n",
       "6          6  Life: Life Of Luxury: Elton John’s 6 Favorite ...   \n",
       "7          7  Benoît Hamon Wins French Socialist Party’s Pre...   \n",
       "8          8  Excerpts From a Draft Script for Donald Trump’...   \n",
       "9          9  A Back-Channel Plan for Ukraine and Russia, Co...   \n",
       "10        10  Obama’s Organizing for Action Partners with So...   \n",
       "11        11  BBC Comedy Sketch \"Real Housewives of ISIS\" Ca...   \n",
       "12        12  Russian Researchers Discover Secret Nazi Milit...   \n",
       "13        13  US Officials See No Link Between Trump and Russia   \n",
       "14        14  Re: Yes, There Are Paid Government Trolls On S...   \n",
       "15        15  In Major League Soccer, Argentines Find a Home...   \n",
       "16        16  Wells Fargo Chief Abruptly Steps Down - The Ne...   \n",
       "17        17  Anonymous Donor Pays $2.5 Million To Release E...   \n",
       "18        18                          FBI Closes In On Hillary!   \n",
       "19        19  Chuck Todd: ’BuzzFeed Did Donald Trump a Polit...   \n",
       "20        20  News: Hope For The GOP: A Nude Paul Ryan Has J...   \n",
       "21        21  Monica Lewinsky, Clinton Sex Scandal Set for ’...   \n",
       "22        22  Rob Reiner: Trump Is ’Mentally Unstable’ - Bre...   \n",
       "23        23  Massachusetts Cop’s Wife Busted for Pinning Fa...   \n",
       "24        24  Abortion Pill Orders Rise in 7 Latin American ...   \n",
       "25        25  Nukes and the UN: a Historic Treaty to Ban Nuc...   \n",
       "26        26  EXCLUSIVE: Islamic State Supporters Vow to ‘Sh...   \n",
       "27        27  Humiliated Hillary Tries To Hide What Camera C...   \n",
       "28        28  Andrea Tantaros of Fox News Claims Retaliation...   \n",
       "29        29  How Hillary Clinton Became a Hawk - The New Yo...   \n",
       "...      ...                                                ...   \n",
       "20770  20770  HUMA ABEDIN SWORE UNDER OATH SHE GAVE UP ‘ALL ...   \n",
       "20771  20771                                                      \n",
       "20772  20772                                                      \n",
       "20773  20773  Australia to hunt down anti-vax nurses and pro...   \n",
       "20774  20774  Government Report: Islamists Building ’Paralle...   \n",
       "20775  20775  How this WWII airman is helping veterans heal ...   \n",
       "20776  20776  Trump Campaign Says Hillary Supporter Tried As...   \n",
       "20777  20777  Editor of Austria’s Largest Paper Charged with...   \n",
       "20778  20778  This Is a Jobs Report That Democrats Can Boast...   \n",
       "20779  20779  Christians in 2017 ’Most Persecuted Group in t...   \n",
       "20780  20780  Florida Woman Charged in Death of Infant in ‘C...   \n",
       "20781  20781  Time is Running Out to Stop Kratom Ban – Need ...   \n",
       "20782  20782  The Fix Is In: NBC Affiliate Accidentally Post...   \n",
       "20783  20783  Samsung, Kim Jong-un, Rex Tillerson: Your Morn...   \n",
       "20784  20784  Comment on World Heaves Sigh of Relief after T...   \n",
       "20785  20785  Ann Coulter: How to Provide Universal Health C...   \n",
       "20786  20786  Government Forces Advancing at Damascus-Aleppo...   \n",
       "20787  20787  Sally Yates Won’t Say If Trump Was Wiretapped ...   \n",
       "20788  20788  Maine’s Gov. LePage Threatens To ‘Investigate’...   \n",
       "20789  20789  Sen. McConnell: The Supreme Court Vacancy Was ...   \n",
       "20790  20790  Nikki Haley Blasts U.N. Human Rights Office fo...   \n",
       "20791  20791  Lawyer Who Kept Hillary Campaign Chief Out of ...   \n",
       "20792  20792  Jakarta Bombing Kills Three Police Officers, L...   \n",
       "20793  20793  Idiot Who Destroyed Trump Hollywood Star Gets ...   \n",
       "20794  20794  Trump: Putin ’Very Smart’ to Not Retaliate ove...   \n",
       "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "20799  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "5                                Daniel Nussbaum   \n",
       "6                                            nan   \n",
       "7                                Alissa J. Rubin   \n",
       "8                                            nan   \n",
       "9                   Megan Twohey and Scott Shane   \n",
       "10                                   Aaron Klein   \n",
       "11                               Chris Tomlinson   \n",
       "12                                 Amando Flavio   \n",
       "13                                    Jason Ditz   \n",
       "14                                  AnotherAnnie   \n",
       "15                                 Jack Williams   \n",
       "16              Michael Corkery and Stacy Cowley   \n",
       "17                                      Starkman   \n",
       "18                                       The Doc   \n",
       "19                                     Jeff Poor   \n",
       "20                                           nan   \n",
       "21                                 Jerome Hudson   \n",
       "22                                       Pam Key   \n",
       "23                                           nan   \n",
       "24          Donald G. McNeil Jr. and Pam Belluck   \n",
       "25                                   Ira Helfand   \n",
       "26                     Aaron Klein and Ali Waked   \n",
       "27                                   Amanda Shea   \n",
       "28                                     Jim Dwyer   \n",
       "29                                  Mark Landler   \n",
       "...                                          ...   \n",
       "20770                                 Iron Sheik   \n",
       "20771                                 Letsbereal   \n",
       "20772                                beersession   \n",
       "20773                                Vicki Batts   \n",
       "20774                                Liam Deacon   \n",
       "20775                            Arnaldo Rodgers   \n",
       "20776                             Jameson Parker   \n",
       "20777                                      admin   \n",
       "20778                                 Neil Irwin   \n",
       "20779                  Thomas D. Williams, Ph.D.   \n",
       "20780                           Christine Hauser   \n",
       "20781                          Heather Callaghan   \n",
       "20782                                    The Doc   \n",
       "20783                           Charles McDermid   \n",
       "20784                               Debbie Menon   \n",
       "20785                                Ann Coulter   \n",
       "20786                                        nan   \n",
       "20787                                  Ian Mason   \n",
       "20788                                  Joe Clark   \n",
       "20789                         Warner Todd Huston   \n",
       "20790                                  Adam Shaw   \n",
       "20791                          Daniel Greenfield   \n",
       "20792                               John Hayward   \n",
       "20793                                Robert Rich   \n",
       "20794                              Lee Stranahan   \n",
       "20795                              Jerome Hudson   \n",
       "20796                           Benjamin Hoffman   \n",
       "20797  Michael J. de la Merced and Rachel Abrams   \n",
       "20798                                Alex Ansary   \n",
       "20799                              David Swanson   \n",
       "\n",
       "                                                    text  label  \n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1      Ever get the feeling your life circles the rou...      0  \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1  \n",
       "5      In these trying times, Jackie Mason is the Voi...      0  \n",
       "6      Ever wonder how Britain’s most iconic pop pian...      1  \n",
       "7      PARIS  —   France chose an idealistic, traditi...      0  \n",
       "8      Donald J. Trump is scheduled to make a highly ...      0  \n",
       "9      A week before Michael T. Flynn resigned as nat...      0  \n",
       "10     Organizing for Action, the activist group that...      0  \n",
       "11     The BBC produced spoof on the “Real Housewives...      0  \n",
       "12     The mystery surrounding The Third Reich and Na...      1  \n",
       "13     Clinton Campaign Demands FBI Affirm Trump's Ru...      1  \n",
       "14     Yes, There Are Paid Government Trolls On Socia...      1  \n",
       "15     Guillermo Barros Schelotto was not the first A...      0  \n",
       "16     The scandal engulfing Wells Fargo toppled its ...      0  \n",
       "17     A Caddo Nation tribal leader has just been fre...      1  \n",
       "18     FBI Closes In On Hillary! Posted on Home » Hea...      1  \n",
       "19     Wednesday after   Donald Trump’s press confere...      0  \n",
       "20     Email \\nSince Donald Trump entered the electio...      1  \n",
       "21     Screenwriter Ryan Murphy, who has produced the...      0  \n",
       "22     Sunday on MSNBC’s “AM Joy,” actor and director...      0  \n",
       "23     Massachusetts Cop’s Wife Busted for Pinning Fa...      1  \n",
       "24     Orders for abortion pills by women in seven La...      0  \n",
       "25     Email \\nIn an historic move the United Nations...      1  \n",
       "26     JERUSALEM  —   Islamic State sympathizers and ...      0  \n",
       "27     Humiliated Hillary Tries To Hide What Camera C...      1  \n",
       "28     Andrea Tantaros, a former Fox News host, charg...      0  \n",
       "29     Hillary Clinton sat in the hideaway study off ...      0  \n",
       "...                                                  ...    ...  \n",
       "20770  Home › POLITICS | US NEWS › HUMA ABEDIN SWORE ...      1  \n",
       "20771  DYN's Statement on Last Week's Botnet Attack h...      1  \n",
       "20772  Kinda reminds me of when Carter gave away the ...      1  \n",
       "20773  Australia to hunt down anti-vax nurses and pro...      1  \n",
       "20774  Aided by a politically correct culture of “tol...      0  \n",
       "20775  ‹ › Arnaldo Rodgers is a trained and educated ...      1  \n",
       "20776  Donald Trump was rushed from a rally stage by ...      1  \n",
       "20777  Breitbart October 26, 2016 \\nAn editor of Aust...      1  \n",
       "20778  There’s not much to say about the July jobs nu...      0  \n",
       "20779  In many parts of the world, Christians gatheri...      0  \n",
       "20780  Early on Oct. 6, Erin   was awakened by the so...      0  \n",
       "20781  By Brandon Turbeville When the DEA announced t...      1  \n",
       "20782  Home » Headlines » World News » The Fix Is In:...      1  \n",
       "20783  Good morning.  Here’s what you need to know: •...      0  \n",
       "20784    Finian Cunningham has written extensively on...      1  \n",
       "20785  The first sentence of Congress’ Obamacare repe...      0  \n",
       "20786  #FROMTHEFRONT #MAPS 22.11.2016 - 1,361 views 5...      1  \n",
       "20787  Former Deputy Attorney General Sally Yates dec...      0  \n",
       "20788  Google Pinterest Digg Linkedin Reddit Stumbleu...      1  \n",
       "20789  Senate Majority Leader Mitch McConnell (R, KY)...      0  \n",
       "20790  U. S Ambassador to the United Nations Nikki Ha...      0  \n",
       "20791  Lawyer Who Kept Hillary Campaign Chief Out of ...      1  \n",
       "20792  Two suicide bombers attacked a bus station in ...      0  \n",
       "20793  Share This \\nAlthough the vandal who thought i...      1  \n",
       "20794  Donald Trump took to Twitter Friday to praise ...      0  \n",
       "20795  Rapper T. I. unloaded on black celebrities who...      0  \n",
       "20796  When the Green Bay Packers lost to the Washing...      0  \n",
       "20797  The Macy’s of today grew from the union of sev...      0  \n",
       "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
       "20799    David Swanson is an author, activist, journa...      1  \n",
       "\n",
       "[20800 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train data pre-processing\n",
    "\n",
    "The first step when building a neural network model is getting your data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer. We'll also want to clean it up a bit.\n",
    "\n",
    "You can see an example of the articles data above. Here are the processing steps, we'll want to take:\n",
    ">* We'll want to get rid of periods and extraneous punctuation.\n",
    "* Also, you might notice that the some texts are delimited with newline characters `\\n`. To deal with those, I'm going to split the text into each text using `\\n` as the delimiter. \n",
    "* Then I can combined all the text back together into one big string.\n",
    "\n",
    "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from string import punctuation\n",
    "\n",
    "texts = [''.join([c for c in text.lower() if c not in punctuation]) for text in train_raw_data['text']]\n",
    "\n",
    "# split by new lines and spaces\n",
    "all_text = ' '.join(texts)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Encoding the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our texts into a list of integers so they can be passed into the network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# use the dict to tokenize each article in text split\n",
    "# store the tokenized texts in text_ints\n",
    "text_ints = []\n",
    "for text in texts:\n",
    "    text_ints.append([vocab_to_int[word] for word in text.split()])\n",
    "\n",
    "# get the labels (0 and 1) from the data set\n",
    "encoded_labels = [label for label in train_raw_data['label']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))\n",
    "\n",
    "# print tokens in first article\n",
    "# print('Tokenized text: \\n', text_ints[:1])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Unique words:  267449\n",
      "Tokenized text: \n",
      " [[127, 5765, 2273, 41, 359, 84, 142, 2655, 771, 316, 3069, 7489, 2879, 13, 17, 13053, 27624, 10, 361, 662, 152, 3379, 3069, 7489, 10, 1, 12243, 6, 97, 15543, 4158, 1086, 4627, 737, 254183, 955, 169, 5, 2666, 175729, 2668, 11, 12251, 2, 6091, 46532, 60, 8, 66, 1454, 31, 1, 1539, 426, 6, 1, 107, 8, 26, 193612, 304, 728, 599, 29, 154, 2, 5, 127, 269, 2273, 13, 1327, 62, 41, 64, 128, 31, 1, 261605, 426, 8, 14, 162, 13, 2089, 58, 7, 56, 599, 598, 22, 64581, 771, 4053, 7, 1, 295, 12, 626, 73, 364, 7, 115, 21, 1038, 2, 114, 385, 249, 1403, 1, 5909, 332, 10, 1, 3186, 5331, 359, 1141, 40, 13, 24, 599, 30, 206, 58, 1084, 5, 2258, 24, 48, 3, 1, 194, 478, 13750, 14, 41, 81, 128, 599, 7273, 1, 194, 13750, 4, 269, 5909, 272, 3, 1, 127, 443, 4065, 4, 4200, 5331, 7, 22, 582, 12, 5654, 364, 13, 36, 493, 1442, 6, 266, 2, 142, 54, 30, 3284, 1617, 261, 23, 188, 63, 26, 771, 369, 58, 4200, 478, 858, 3069, 7489, 325, 1, 140, 107, 20551, 11, 26, 2258, 295, 23236, 77, 2665, 131, 1, 295, 27, 1273, 3, 1, 2377, 3, 364, 7, 1145, 2, 21, 8730, 2, 1, 311, 213, 5089, 35, 3069, 7489, 38764, 361, 1189, 152, 3, 388, 41, 81, 128, 7, 26, 12, 23, 1, 213, 599, 12, 392, 297, 7, 13, 12, 5654, 1, 364, 6, 843, 3, 3098, 7292, 218724, 41, 81, 128, 2, 21, 2066, 5023, 8770, 11, 5, 4704, 29, 1395, 156, 263, 247, 14, 1735, 359, 437, 2, 7489, 1, 4158, 194, 36, 260, 3431, 2, 10898, 5, 15350, 3, 1989, 54, 114, 262069, 270, 93, 15457, 1108, 4, 1763, 28, 696, 42782, 1108, 3, 74, 1395, 7489, 467, 1, 295, 12, 260, 384, 22, 163, 9, 185011, 6, 5, 2258, 7, 4177, 17336, 1, 576, 108, 11301, 2485, 3023, 13, 12, 5, 37609, 29, 154, 2, 5, 577, 127, 269, 2273, 43748, 7, 771, 115, 20, 46, 1, 270, 3, 119365, 10058, 7, 2273, 147, 23161, 7, 22, 3367, 4, 69, 332, 359, 84, 128, 40, 2655, 771, 19, 1, 181732, 91, 206, 58, 56, 30, 5103, 210, 26780, 5909, 272, 10, 1, 3186, 5331, 359, 1495, 2655, 771, 316, 63, 1, 194, 13750, 6, 279, 1, 269, 5909, 272, 80814, 1495, 13, 316, 63, 1, 858, 3, 1, 4200, 4, 98, 1942, 478, 3069, 7489, 2879, 13, 58, 4, 125, 13, 13983, 59, 1540, 142, 54, 1207, 301, 26, 173, 1, 295, 304, 2169, 7489, 4, 69, 1866, 478, 13750, 40, 5, 355, 1021, 6, 5, 1952, 2171, 4331, 311, 4, 1661, 7489, 1287, 22, 69, 1976, 36, 1, 4627, 2, 533, 37, 269, 6085, 128, 40, 13, 413, 154, 2, 26, 2273, 15, 125, 74, 313, 58, 40, 13, 10, 210, 60, 27, 260, 46, 570, 10, 592, 22987, 7, 599, 444, 1061, 2131, 2383, 3, 26, 771, 2, 7489, 4, 69, 354, 1088, 74, 79, 2, 628, 10, 1, 5819, 1847, 7, 115, 123, 9, 186, 2306, 29, 60, 8, 403, 59, 253, 7, 84, 2047, 26, 8, 1, 213, 63, 52, 60, 8, 403, 59, 253, 7, 2047, 7, 599, 12, 465, 69, 68, 12901, 9927, 4, 60322, 53, 13, 217, 1893, 299, 8, 7, 7489, 8, 1805, 6, 5, 124, 7, 597, 3518, 9416, 4, 13053, 18706, 276, 62, 3671, 3, 1649, 4, 32718, 15, 359, 84, 20, 1, 11852, 2, 8782, 5909, 610, 18923, 18944, 40, 239, 26, 4331, 54, 7, 453, 24266, 10, 1801, 1858, 3, 6943, 32, 136, 128, 53, 217, 2795, 99, 23, 358, 7, 7489, 42, 20, 2, 1080, 9, 26, 15, 4940, 6, 5, 14370, 194, 1008, 16399, 6, 46823, 4, 117968, 13, 27, 5, 3583, 3436, 623, 4629, 3, 140807, 4, 722, 4539, 3050, 5, 10658, 7707, 165, 3, 1, 240, 6, 721, 3316, 1, 194, 127, 1093, 27, 366, 57, 501, 195, 2, 119365, 1006, 6179, 16321, 29, 7, 453, 725, 41, 452, 628, 1, 2243, 3817, 10, 80, 63, 52, 15, 8, 5, 14492, 553, 3, 53, 1, 127, 27, 271, 169, 194, 336, 4, 15, 8, 64, 1, 386, 1539, 426, 6, 1, 107, 40, 13053, 27624, 13053, 8, 5, 40999, 3391, 3, 1, 315, 3, 290, 1290, 31, 4978, 444, 5, 1767, 3, 1, 487, 282, 28, 1171, 2, 628, 80, 73, 5, 610, 3, 1, 1105, 173, 6, 600, 91, 4987, 6, 1798, 80, 73, 1, 1105, 312, 1539, 40859, 11546, 1411, 31, 8, 28, 15131, 1004, 22, 2157, 2, 874, 65, 9, 96, 31, 20, 46, 4410, 73, 2983, 91, 1309, 56, 15, 3726, 28, 8278, 25152, 2013, 33, 115, 128, 80, 10, 592, 22987, 14, 1411, 5765, 6, 9652, 378, 80, 10, 210, 40066, 38, 4217, 11, 80, 10, 417, 1728, 146, 2, 1075, 13053, 5, 21593, 37896, 4217]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# outlier article stats\n",
    "text_lens = Counter([len(x) for x in text_ints])\n",
    "print(\"Zero-length text: {}\".format(text_lens[0]))\n",
    "print(\"Maximum text length: {}\".format(max(text_lens)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Zero-length text: 116\n",
      "Maximum text length: 24195\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We seem to have some texts with zero length. And, the maximum text length is way too many steps for our RNN. We'll have to remove any super short texts and truncate super long texts. This removes outliers and should allow our model to train more efficiently."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print('Number of texts before removing outliers: ', len(text_ints))\n",
    "\n",
    "## remove any articles/labels with zero length from the text_ints list.\n",
    "\n",
    "# get indices of any articles with length 0\n",
    "non_zero_idx = [ii for ii, text in enumerate(text_ints) if len(text) != 0]\n",
    "\n",
    "# remove 0-length articles and their labels\n",
    "text_ints = [text_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of texts after removing outliers: ', len(text_ints))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of texts before removing outliers:  20800\n",
      "Number of texts after removing outliers:  20684\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long articles, we'll pad or truncate all our articles to a specific length. For texts shorter than some `seq_length`, we'll pad with 0s. For texts longer than `seq_length`, we can truncate them to the first `seq_length` words. A good `seq_length`, in this case, is 200.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/minhkhang1795/FakeNews_RNN/master/assets/outliers_padding_ex.png\" width=40%>\n",
    "\n",
    "> Define a function that returns an array `features` that contains the padded data, of a standard size, that we'll pass to the network. \n",
    "* The data should come from `text_ints`, since we want to feed integers to the network. \n",
    "* Each row should be `seq_length` elements long. \n",
    "* For articles shorter than `seq_length` words, **left pad** with 0s. That is, if the text is `['best', 'movie', 'ever']`, `[117, 18, 128]` as integers, the row will look like `[0, 0, 0, ..., 0, 117, 18, 128]`. \n",
    "* For articles longer than `seq_length`, use only the first `seq_length` words as the feature vector.\n",
    "\n",
    "As a small example, if the `seq_length=10` and an input article is: \n",
    "```\n",
    "[117, 18, 128]\n",
    "```\n",
    "The resultant, padded sequence should be: \n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
    "```\n",
    "\n",
    "**The final `features` array should be a 2D array, with as many rows as there are articles, and as many columns as the specified `seq_length`.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def pad_features(text_ints, seq_length):\n",
    "    ''' Return features of text_ints, where each article is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(text_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each article, grab that article and \n",
    "    for i, row in enumerate(text_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "seq_length = 200\n",
    "\n",
    "features = pad_features(text_ints, seq_length=seq_length)\n",
    "\n",
    "# test statements - do not change -\n",
    "assert len(features)==len(text_ints), \"Your features should have as many rows as articles.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30, :10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[   127   5765   2273     41    359     84    142   2655    771    316]\n",
      " [   365    113      1   1720     89    201   5177      1  30633    522]\n",
      " [   202      1    742    209    113     33   1461    361   1851    152]\n",
      " [  1538    686   1271    473      6    786     67   6721     20     46]\n",
      " [     0      0      0      0      0      0      0      0      0      0]\n",
      " [     6    102    380    167  10178   7951      8      1   1258      3]\n",
      " [   365   2235     87   3106     90   8397   3051  15953   1090    141]\n",
      " [  1413     35   1202   2557     28  19928   1409    435      6   6511]\n",
      " [   159    488     39      8   1979      2    123      5   1401   6742]\n",
      " [     5    212    108    737   1638   2534   4530     14    150    193]\n",
      " [  4364      9    601      1   2207    200      7  18244     24   1019]\n",
      " [     1   3513   1842  32903     10      1   9264  52757   1227  21333]\n",
      " [     1   4507   2591      1    781  10869      4   3961   1210      8]\n",
      " [    75    119   2629    295  15074   3335    182   1372     11      1]\n",
      " [  1130     60     18    817     98  11372     10    298    135  10901]\n",
      " [ 27696 165664  52672     12     23      1     94  15645   2023      2]\n",
      " [     1   1643  41746   3026   4542  13030     57    858      4    347]\n",
      " [     5  39271    576   4576    511     27     77     46   7403     63]\n",
      " [   295  13176      6     10    114    659     10    221   3123   3429]\n",
      " [   451     63    159    143    454    798     19     39   2519      6]\n",
      " [   249    151    159     39   2342      1    122     76      5    116]\n",
      " [ 21486    994   6777     31     27   1842      1  18556    673     97]\n",
      " [   573     10   8001  17667  22306   2686      4    304   5943  40518]\n",
      " [  3145  40660    762  12486      9  31245   1365 215128   6889     10]\n",
      " [  2004      9   1876   7384     17    205      6    996   3585     97]\n",
      " [   249      6     28   2529    445      1    101    580     94    478]\n",
      " [  2026     35    462     85  17417      4   2482   3808    100   4394]\n",
      " [ 13083    114   4123      2   3356     53   2527   1491    686  11007]\n",
      " [  9675  19396      5    172    775    105   1040   1302      6      5]\n",
      " [   114     75   2390      6      1 117288    596    175     49  13224]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training, Validation, Test\n",
    "\n",
    "With our data in nice shape, we'll split it into training, validation, and test sets.\n",
    "\n",
    "> Create the training, validation, and test sets from the `train_raw_data`\n",
    "* You'll need to create sets for the features and the labels, `train_x` and `train_y`, for example. \n",
    "* Define a split fraction, `split_frac` as the fraction of data to **keep** in the training set. Usually this is set to 0.8 or 0.9. \n",
    "* Whatever data is left will be split in half to create the validation and *testing* data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# split data into training, validation, and test data (features and labels, x and y)\n",
    "split_idx = 16000 # about 80%\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = (len(features)-16000)//2\n",
    "val_x, test_x = remaining_x[:test_idx-2], remaining_x[test_idx-2:-4]\n",
    "val_y, test_y = remaining_y[:test_idx-2], remaining_y[test_idx-2:-4]\n",
    "\n",
    "# print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(16000, 200) \n",
      "Validation set: \t(2340, 200) \n",
      "Test set: \t\t(2340, 200)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, we can create **DataLoaders** for this data by following two steps:\n",
    "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
    "\n",
    "```\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "This is an alternative to creating a generator function for batching our data into full batches."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 10\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=5)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size, num_workers=5)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, num_workers=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size())  # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size())  # batch_size\n",
    "print('Sample label: \\n', sample_y)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sample input size:  torch.Size([10, 200])\n",
      "Sample input: \n",
      " tensor([[     0,      0,      0,  ...,   1786,   1266,    152],\n",
      "        [     0,      0,      0,  ...,   2590,    236,   8055],\n",
      "        [   202,    159,     39,  ...,    560,     68,    588],\n",
      "        ...,\n",
      "        [ 18616,   1502,   2000,  ...,  20006,      2,     49],\n",
      "        [212536, 218481,      1,  ...,   2308,      4,     32],\n",
      "        [     1,   2347,      3,  ...,   1243,    310,     24]])\n",
      "\n",
      "Sample label size:  torch.Size([10])\n",
      "Sample label: \n",
      " tensor([1, 1, 1, 0, 1, 0, 0, 1, 1, 0])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Recurrent Neural Network with PyTorch\n",
    "\n",
    "Below is where we'll define the network.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/minhkhang1795/FakeNews_RNN/master/assets/network_diagram.png\" width=40%>\n",
    "\n",
    "The layers are as follows:\n",
    "1. An [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) that converts our word tokens (integers) into embeddings of a specific size.\n",
    "2. An [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) defined by a hidden_state size and number of layers\n",
    "3. A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n",
    "4. A sigmoid activation layer which turns all outputs into a value 0-1; return **only the last sigmoid output** as the output of this network.\n",
    "\n",
    "### The Embedding Layer\n",
    "\n",
    "We need to add an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) because there are 267449+ words in our vocabulary. It is massively inefficient to one-hot encode that many classes. So, instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. You could train an embedding layer using Word2Vec, then load it here. But, it's fine to just make a new layer, using it for only dimensionality reduction, and let the network learn the weights.\n",
    "\n",
    "\n",
    "### The LSTM Layer(s)\n",
    "\n",
    "We'll create an [LSTM](https://pytorch.org/docs/stable/nn.html#lstm) to use in our recurrent network, which takes in an input_size, a hidden_dim, a number of layers, a dropout probability (for dropout between multiple layers), and a batch_first parameter.\n",
    "\n",
    "Most of the time, our network will have better performance with more layers; between 2-3. Adding more layers allows the network to learn really complex relationships. \n",
    "\n",
    "Note: `init_hidden` should initialize the hidden and cell state of an lstm layer to all zeros, and move those state to GPU, if available."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]  # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" Initializes hidden state \"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int) + 1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 200\n",
    "hidden_dim = 256\n",
    "n_layers = 3\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "# move model to GPU, if available\n",
    "if train_on_gpu:\n",
    "    net.cuda()\n",
    "    \n",
    "print(net)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(267450, 200)\n",
      "  (lstm): LSTM(200, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Below is the typical training code. We'll also be using a new kind of cross entropy loss, which is designed to work with a single Sigmoid output. [BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss), or **Binary Cross Entropy Loss**, applies cross entropy loss to a single value between 0 and 1.\n",
    "\n",
    "We also have some data and training hyparameters:\n",
    "\n",
    "* `lr`: Learning rate for our optimizer.\n",
    "* `epochs`: Number of times to iterate through the training dataset.\n",
    "* `clip`: The maximum gradient value to clip at (to prevent exploding gradients)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# loss and optimization functions\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=1, verbose=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "state_dict = torch.load('checkpoint5_9329.pth')\n",
    "net.load_state_dict(state_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# training params\n",
    "epochs = 20\n",
    "clip = 5  # gradient clipping\n",
    "min_loss = np.inf\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    num_correct = 0\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    net.train()\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "    # Get validation loss\n",
    "    val_h = net.init_hidden(batch_size)\n",
    "    net.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        output, val_h = net(inputs, val_h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "        # convert output probabilities to predicted class (0 or 1)\n",
    "        pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
    "          \"Loss: {:.6f}...\".format(train_loss),\n",
    "          \"Val Loss: {:.6f}\".format(valid_loss),\n",
    "          \"Accuracy: {:.6f}\".format(num_correct/len(valid_loader.dataset)))\n",
    "    if min_loss >= valid_loss:\n",
    "        torch.save(net.state_dict(), 'checkpointx.pth')\n",
    "        min_loss = valid_loss\n",
    "        print(\"Loss decreased. Saving model...\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 1/20... Loss: 0.159931... Val Loss: 0.162035 Accuracy: 0.935470\n",
      "Loss decreased. Saving model...\n",
      "Epoch: 2/20... Loss: 0.091499... Val Loss: 0.182338 Accuracy: 0.934188\n",
      "Epoch     2: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 3/20... Loss: 0.048477... Val Loss: 0.226819 Accuracy: 0.941880\n",
      "Epoch: 4/20... Loss: 0.022922... Val Loss: 0.224598 Accuracy: 0.940598\n",
      "Epoch     4: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 5/20... Loss: 0.014395... Val Loss: 0.254986 Accuracy: 0.939744\n",
      "Epoch: 6/20... Loss: 0.010363... Val Loss: 0.261356 Accuracy: 0.940598\n",
      "Epoch     6: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 7/20... Loss: 0.008913... Val Loss: 0.267870 Accuracy: 0.941026\n",
      "Epoch: 8/20... Loss: 0.008491... Val Loss: 0.268791 Accuracy: 0.940598\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 9/20... Loss: 0.009136... Val Loss: 0.269656 Accuracy: 0.940598\n",
      "Epoch: 10/20... Loss: 0.009926... Val Loss: 0.269735 Accuracy: 0.940171\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 11/20... Loss: 0.010135... Val Loss: 0.269794 Accuracy: 0.940171\n",
      "Epoch: 12/20... Loss: 0.009945... Val Loss: 0.269793 Accuracy: 0.940171\n",
      "Epoch: 13/20... Loss: 0.008764... Val Loss: 0.269795 Accuracy: 0.940171\n",
      "Epoch: 14/20... Loss: 0.008573... Val Loss: 0.269796 Accuracy: 0.940171\n",
      "Epoch: 15/20... Loss: 0.008386... Val Loss: 0.269798 Accuracy: 0.940171\n",
      "Epoch: 16/20... Loss: 0.010016... Val Loss: 0.269799 Accuracy: 0.940171\n",
      "Epoch: 17/20... Loss: 0.008915... Val Loss: 0.269801 Accuracy: 0.940171\n",
      "Epoch: 18/20... Loss: 0.008018... Val Loss: 0.269803 Accuracy: 0.940171\n",
      "Epoch: 19/20... Loss: 0.009368... Val Loss: 0.269805 Accuracy: 0.940171\n",
      "Epoch: 20/20... Loss: 0.009631... Val Loss: 0.269807 Accuracy: 0.940171\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# torch.save(net.state_dict(), 'checkpoint6.pth')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data splitted above. The best accuracy this RNN can get is **93.29%**.\n",
    "\n",
    "* **Performance on real test set from Kaggle:** Second, we'll see our RNN performance on the real test set from Kaggle. My submission resulted in **92.09%** for private score and **91.41%** for public score."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# Get test data loss and accuracy\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "loader = test_loader\n",
    "# iterate over test data\n",
    "for inputs, labels in loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if train_on_gpu:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(loader.dataset)\n",
    "print(\"Test accuracy: {:.4f}%\".format(test_acc*100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test loss: 0.316\n",
      "Test accuracy: 93.2906%\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction on Kaggle test set\n",
    "**test.csv**: A testing training dataset with all the same attributes at **train.csv** without the label."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "test_fpath = 'test.csv'\n",
    "test_raw_data = pd.read_csv(test_fpath, encoding='utf-8', header=0, keep_default_na=False)\n",
    "\n",
    "# Only keep title and text\n",
    "test_data = []\n",
    "for i in range(len(test_raw_data)):\n",
    "    test_data.append(test_raw_data['title'][i] + ' ' + test_raw_data['text'][i])\n",
    "# test_data[:10]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Specter of Trump Loosens Tongues, if Not Purse Strings, in Silicon Valley - The New York Times PALO ALTO, Calif.  —   After years of scorning the political process, Silicon Valley has leapt into the fray. The prospect of a President Donald J. Trump is pushing the tech community to move beyond its traditional role as donors and to embrace a new existence as agitators and activists. A distinguished venture capital firm emblazoned on its corporate home page an earthy   epithet. One prominent tech chieftain says the consequences of Mr. Trump’s election would “range between disastrous and terrible. ” Another compares him to a dictator. And nearly 150 tech leaders signed an open letter decrying Mr. Trump and his campaign of “anger” and “bigotry. ” Not quite all the action is  . Peter Thiel, a founder of PayPal and Palantir who was the first outside investor in Facebook, spoke at the Republican convention in July. The New York Times reported on Saturday that Mr. Thiel is giving $1. 25 million to support Mr. Trump’s candidacy even as other supporters flee. (He also recently gave $1 million to a “super PAC” that supports Senator Rob Portman, the Republican freshman running for   in Ohio.) Getting involved in politics used to be seen as clashing with Silicon Valley’s value system: You transform the world by making problems obsolete, not solving them through Washington. Nor did entrepreneurs want to alienate whatever segment of customers did not agree with them politically. Such reticence is no longer in style here. “We’re a bunch of nerds not used to having a lot of limelight,” said Dave McClure, an investor who runs a tech incubator called 500 Startups. “But to quote   ‘With great power comes great responsibility. ’” Mr. McClure grew worried after the Republican and Democratic conventions as Mr. Trump began to catch up to Hillary Clinton in the polls. He wanted Silicon Valley to do more, and so late last month he announced Nerdz4Hillary, an informal   effort. An initial group of donors pledged $50, 000 the goal was to ask the “nerdz” for small donations to match that sum. They have not come through yet. “We’re kind of optimistic we’ll get the other $50, 000 in a few weeks,” Mr. McClure said. That relatively slow pace reflects Silicon Valley’s shifting position: Even as it becomes increasingly free with its opinions, it has been less free with its checkbook. The most recent data, from late August, shows Mrs. Clinton taking in $7. 7 million from the tech community, according to Crowdpac, a   that tracks donations. By that point in 2012, Crowdpac says, President Obama had raised $21 million from entrepreneurs and venture capitalists. Reid Hoffman, the billionaire   of the business networking site LinkedIn, offers a snapshot of Silicon Valley’s evolving approach to politics. Mr. Hoffman was a top Obama donor, giving $1 million to the Priorities USA political action committee, something several of his peers did as well. Last month, Mr. Hoffman garnered worldwide publicity for saying he would donate up to $5 million to veterans’ groups if Mr. Trump released his taxes, a remote possibility that never came to pass. He has castigated Mr. Trump in interviews, saying he was speaking for those who were afraid. Mr. Hoffman’s outright donations, however, have been smaller this election cycle. In May, he gave $400, 000 to the Hillary Victory Fund. Asked if there was more recent giving that had not shown up in federal election records, Mr. Hoffman cryptically responded in an email, “Looking at some PACs, etc. ” He declined several opportunities to elaborate. Even as Priorities USA has raised $133 million this election cycle, far exceeding its total in 2012, its tech contributions have dwindled. The only familiar tech name this time around is John Doerr of the venture capital firm Kleiner Perkins Caufield  Byers, who gave $500, 000. The AOL   Steve Case said his September endorsement of Mrs. Clinton, via an   in The Washington Post, was the first time he ever publicly declared for a candidate. “I always focused on policy and avoided politics,” he said. “But if Trump were elected president, I would be disappointed in myself for not acting. ” When he wrote the   he was uncertain about donating money to Mrs. Clinton, saying only that it was “probable. ” A spokeswoman said Sunday that Mr. Case gave $25, 000 to the Hillary Victory Fund. Mason Harrison, Crowdpac’s head of communications, offered a possible reason for Mrs. Clinton’s    support. “Donors give to support candidates they love, not to defeat candidates they fear,” he said. A few billionaires are acting instead of talking. Dustin Moskovitz, a founder of Facebook, said he was giving $20 million to various Democratic election efforts  —   the first time he and his wife, Cari Tuna, have endorsed a candidate. He declined to be interviewed. Part of the problem for Mrs. Clinton is that, however preferable she may be to Mr. Trump in the tech community, she pales in comparison to President Obama. After some initial misgivings, Silicon Valley found its champion in him. There has been a revolving door between tech and the Obama administration, just as previous Democratic administrations had a revolving door with Wall Street. In June, President Obama seemed to suggest that he might become a venture capitalist after his term ends. Mrs. Clinton is not as enthusiastic toward Silicon Valley and its disruptive ways. In a speech in the summer of 2015, she noted that   in the “  or   gig economy”  —   Uber, Airbnb and their ilk  —   were “unleashing innovation” but also “raising hard questions about workplace protection and what a good job will look like in the future. ” The Clinton campaign declined to comment. The Trump campaign did not respond to a query. Even as Silicon Valley works against Mr. Trump, there is quiet acknowledgment that his campaign has bared some important issues. In an endorsement this month of Mrs. Clinton, the venture capital firm Union Square Ventures pointed out that “the benefits of technology and globalization have not been evenly distributed,” and that this needed to change. If Silicon Valley’s political involvement outlasts this unusual election, the tech community may start contributing more to the process than commentary and cash. “Not only are tech people going to be wielding influence, but they’re going to be the candidate,” Mr. McClure said. “Reid Hoffman, Sheryl Sandberg”  —   the chief operating officer of Facebook  —   “and a bunch of other folks here have political aspirations. ” Others may be inspired to enter politics through other doors. Palmer Luckey is the    founder of the Oculus virtual reality company, which he sold to Facebook for $2 billion. Mr. Luckey donated $10, 000 to a group dedicated to spreading    messages about Mrs. Clinton both online and off. The group’s first billboard, said to be outside Pittsburgh, labeled her “Too Big to Jail. ” Mr. Luckey told The Daily Beast that his thinking “went along the lines of, ‘Hey, I have a bunch of money. I would love to see more of this stuff. ’” He added, “I thought it sounded like a real jolly good time. ” Many virtual reality developers were less happy, and Mr. Luckey quickly posted his regrets on Facebook. He declined to comment further. “If we’re going to be more vocal, we’ll have to live more transparently,” said Hunter Walk, a venture capitalist whose campaign to persuade tech companies to give workers Election Day off signed up nearly 300 firms, including Spotify, SurveyMonkey and TaskRabbit. “There will be a period of adjustment. ” But perhaps being vocal is a temporary condition after all. The venture firm CRV was in the spotlight at the end of August with its blunt   message, which included the earthy epithet. A few weeks later, it cleaned up its website. The partners went from employing a publicist to seek out attention to declining interviews. “We reached everyone we wanted to reach, and hopefully influenced opinions,” said Saar Gur, a CRV venture capitalist. “Then the buzz died down and we went back to our day jobs, which are super busy. ”',\n",
       " 'Russian warships ready to strike terrorists near Aleppo Russian warships ready to strike terrorists near Aleppo 08.11.2016 | Source: Source: Mil.ru Attack aircraft of the Russian aircraft carrier Admiral Kuznetsov get ready to strike terrorists\\' positions in the vicinity of Aleppo, sources at the Russian Defense Ministry said, RBC reports. \"Insurgents\\' attempts to break into Aleppo from outside are meaningless,\" the source said. The main task of the aircraft carrier aviation group is to strike missile and air blows on the terrorists , whose goal is to enter Aleppo. \"After the attacks on terrorists\\' positions, one will have to forget about the support for insurgents from the outside,\" the source said. The Russian group in the Mediterranean Sea consists of the Admiral Kuznetsov aircraft carrier , the heavy nuclear missile cruiser Pyotr Velikiy (Peter the Great) and large anti-submarine ships Severomorsk and Vice-Admiral Kulakov. Russia has increased intelligence activities in Syria to establish the areas, where terrorists are concentrated, as well as the routes that they use to move from one area to another. \"The militants took advantage of the humanitarian pause and regrouped their forces to prepare for a new breakthrough into the eastern part of Aleppo,\" the source added. According to the source, Russia will use new weapons during the upcoming attacks on terrorists . It was said that the Russian warships in the Mediterranean Sea will launch \"Caliber\" cruise missiles, although it was not specified which ships would be responsible for the launches. Pravda.Ru Russian warships travel to Syria',\n",
       " '#NoDAPL: Native American Leaders Vow to Stay All Winter, File Lawsuit Against Police Videos #NoDAPL: Native American Leaders Vow to Stay All Winter, File Lawsuit Against Police Amnesty International are sending a delegation of human rights observers to monitor the response of law enforcement to the protests. Be Sociable, Share! (Rob Wilson photo) \\nNative American leaders vowed on Saturday to protest through the winter against a North Dakota oil pipeline they say threatens water resources and sacred lands and are planning lawsuits over police treatment of arrested protesters. \\nStanding Rock Sioux Chairman Dave Archambault II said he and other tribal leaders were working on providing food, heat and shelter for protesters opposed to the $3.8 billion Dakota Access Pipeline. \\n“We’re just working through some technical details as far as where the land is, and the type of land that can be used for some permanent structures,” Archambault told reporters in Mandan, North Dakota on Saturday morning. \\nAt least 10 shelters were being readied on tribal land against temperatures that can fall below -35 Fahrenheit (-37 Celsius) for days at time, he said. \\n“It doesn’t have to put our water at risk,” said Archambault, who was joined by Cheyenne River Sioux Chairman Harold Frazier. \\nThe two leaders said they’re considering taking legal action against law enforcement. Standing Rock Chairman Dave Archambault II said more than 40 people were injured, including broken bones and welts from rubber bullets and bean bag rounds fired by law enforcement on Thursday, Oct. 27th. \\nArchambault said his tribe may pursue a class action over police tactics. Officers in riot gear swept through a protester camp on private land using pepper spray, bean bag rounds and an audio cannon aiming high-pitched “sound cannon” blasts against demonstrators. At least 142 people were arrested on Thursday and Friday. \\nProtesters had numbers written on their arms and were housed in what appeared to be dog kennels, without bedding or furniture. \\n“It’s just wrong to use that type of force on innocent people,” Archambault said Saturday, Oct. 29, during a press conference in front of the Morton County Sheriff’s Department. \\nCheyenne River Sioux Chairman Harold Frazier said he has heard reports of inhumane treatment while people were incarcerated. \\n“All they’re doing is standing up to protect that water,” Frazier said. Here come reinforcements! As police attack & arrest land protectors thousands of buffalo storm #StandingRock https://t.co/NoQyIsWbxv #NoDAPL pic.twitter.com/oMa647HviB \\n— RoseAnn DeMoro (@RoseAnnDeMoro) October 28, 2016 \\nMeanwhile, Amnesty International USA (AIUSA) announced that they are sending a delegation of human rights observers to monitor the response of law enforcement to the protests. \\nAIUSA also has sent a letter to the Morton County Sheriff’s Department expressing concern about the degree of force used against the protests. The organization will also call on the Department of Justice to investigate police practices. \\nAIUSA sent a delegation of observers to the area in August and has stayed in contact both with the Indigenous community and those policing the protests since then. Letters had previously been sent to the North Dakota Highway Patrol and the Morton County Sheriff’s office calling for law enforcement officers to respect international human rights standards on the policing of protests. \\n“Our observers are here to ensure that everyone’s human rights are protected,” said Eric Ferrero, director of communications for AIUSA. “We’re deeply concerned about what we heard during our previous visit to Standing Rock and what has been reported to us since.” \\nIn some instances, police have responded to protesters with pepper spray and bean bags, and in one instance, private security staff used guard dogs. Those recently arrested have reported being strip searched and forced to pay bail for minor offenses. Members of the media and legal observers have also been arrested or charged with minor offenses. \\n“People here just want to stand up for the rights of Indigenous people and protect their natural resources. These people should not be treated like the enemy,” said Ferrero “Police must keep the peace using minimal force appropriate to the situation. Confronting men, women, and children while outfitted in gear more suited for the battlefield is a disproportionate response.” \\nThis picture should be shared far & wide. I can only see wrong doing from one side and it isn’t the elder! Bullying cretins! #NoDAPL pic.twitter.com/avNjgfYGnz \\n— Crystal Johnson (@Crystal1Johnson) October 29, 2016',\n",
       " 'Tim Tebow Will Attempt Another Comeback, This Time in Baseball - The New York Times If at first you don’t succeed, try a different sport. Tim Tebow, who was a Heisman   quarterback at the University of Florida but was unable to hold an N. F. L. job, is pursuing a career in Major League Baseball. He will hold a workout for M. L. B. teams this month, his agents told ESPN and other news outlets. “This may sound like a publicity stunt, but nothing could be further from the truth,” said Brodie Van Wagenen,   of CAA Baseball, part of the sports agency CAA Sports, in the statement. “I have seen Tim’s workouts, and people inside and outside the industry  —   scouts, executives, players and fans  —   will be impressed by his talent. ” It’s been over a decade since Tebow, 28, has played baseball full time, which means a comeback would be no easy task. But the former major league catcher Chad Moeller, who said in the statement that he had been training Tebow in Arizona, said he was “beyond impressed with Tim’s athleticism and swing. ” “I see bat speed and power and real baseball talent,” Moeller said. “I truly believe Tim has the skill set and potential to achieve his goal of playing in the major leagues and based on what I have seen over the past two months, it could happen relatively quickly. ” Or, take it from Gary Sheffield, the former   outfielder. News of Tebow’s attempted comeback in baseball was greeted with skepticism on Twitter. As a junior at Nease High in Ponte Vedra, Fla. Tebow drew the attention of major league scouts, batting . 494 with four home runs as a left fielder. But he ditched the bat and glove in favor of pigskin, leading Florida to two national championships, in 2007 and 2009. Two former scouts for the Los Angeles Angels told WEEI, a Boston radio station, that Tebow had been under consideration as a high school junior. “’x80’x9cWe wanted to draft him, ’x80’x9cbut he never sent back his information card,” said one of the scouts, Tom Kotchman, referring to a questionnaire the team had sent him. “He had a strong arm and had a lot of power,” said the other scout, Stephen Hargett. “If he would have been there his senior year he definitely would have had a good chance to be drafted. ” “It was just easy for him,” Hargett added. “You thought, If this guy dedicated everything to baseball like he did to football how good could he be?” Tebow’s high school baseball coach, Greg Mullins, told The Sporting News in 2013 that he believed Tebow could have made the major leagues. “He was the leader of the team with his passion, his fire and his energy,” Mullins said. “He loved to play baseball, too. He just had a bigger fire for football. ” Tebow wouldn’t be the first athlete to switch from the N. F. L. to M. L. B. Bo Jackson had one   season as a Kansas City Royal, and Deion Sanders played several years for the Atlanta Braves with mixed success. Though Michael Jordan tried to cross over to baseball from basketball as a    in 1994, he did not fare as well playing one year for a Chicago White Sox minor league team. As a football player, Tebow was unable to match his college success in the pros. The Denver Broncos drafted him in the first round of the 2010 N. F. L. Draft, and he quickly developed a reputation for clutch performances, including a memorable   pass against the Pittsburgh Steelers in the 2011 Wild Card round. But his stats and his passing form weren’t pretty, and he spent just two years in Denver before moving to the Jets in 2012, where he spent his last season on an N. F. L. roster. He was cut during preseason from the New England Patriots in 2013 and from the Philadelphia Eagles in 2015.',\n",
       " \"Keiser Report: Meme Wars (E995) 42 mins ago 1 Views 0 Comments 0 Likes 'For the first time in history, we’re filming a panoramic video from the station. It means you’ll see everything we see here, with your own eyes. That’s to say, you’ll be able to feel like real cosmonauts' - Borisenko to RT. Video presented by RT in collaboration with the Russian space agency Roscosmos and the rocket and space corporation Energia More on our project website: space360.rt.com   Subscribe   Like     Leave a Reply Login with your Social ID Your email address will not be published. Name\",\n",
       " 'Trump is USA\\'s antique hero. Clinton will be next president Trump is USA\\'s antique hero. Clinton will be next president 08.11.2016 | Source: AP photo FBI Director James Comey said on November 6 that his department would not be criminally charging Hillary Clinton for revelations found in her email correspondence. Earlier, however, the FBI had flagged Clinton\\'s email case as a file of high priority. It was said that the FBI had collected a lot of evidence. All of a sudden, it was announced that Clinton would be cleared. Pravda.Ru asked political scientist and publicist Leonid Krutakov to comment on such a development. \"Was it a though-out move to show that Clinton is not guilty?\" \"Did you expect a criminal case against Clinton right before the election? I thought of such a plan if Trump were winning, but now that they have cleared her name, it means that Hillary Clinton will win the election. One can be sure for 100 percent that Hillary Clinton will be the next President of the United States of America. \"I do not think that the FBI Director made that decision independently. This is a political figure, this is a job to which people are appointed by someone else. It is stupid to believe that one FBI director will stand up against the whole elite that prints money and runs business and international politics. The times of ancient heroes have passed. Trump has tried to become one. He has proclaimed a new era, not only in America but in the world, and the electorate that has consolidated around Trump will not go anywhere after the election. \"One needs another war and another threat to America to make Trump\\'s electorate change their mind. As we remember, George W. Bush won the election in no less controversial election battles, when votes in Florida were recounted manually. We remember what happened afterwards - we had September 11, wars in Afghanistan and Iraq, and Bush immediately scored 96 percent of support of the nation. History repeats itself.\" Pravda.Ru Read article on the Russian version of Pravda.Ru What does Hillary Clinton like about Putin?',\n",
       " 'Pelosi Calls for FBI Investigation to Find Out ’What the Russians Have on Donald Trump’ - Breitbart Sunday on NBC’s “Meet the Press,” House Minority Leader Rep. Nancy Pelosi ( )  called for a FBI investigation to find out “what the Russians have” on President Donald Trump.  Pelosi said, “I want to know what the Russians have on Donald Trump. I think we have to have that investigation by the FBI into his financial, personal and political connections to Russia, and we want to see his tax returns so we can have a truth in the relationship between Putin whom he admires. ” Follow Pam Key on Twitter @pamkeyNEN',\n",
       " 'Weekly Featured Profile – Randy Shannon You are here: Home / *Articles of the Bound* / Weekly Featured Profile – Randy Shannon Weekly Featured Profile – Randy Shannon October 31, 2016, 7:21 am by Trevor Loudon Leave a Comment 0 \\nKeyWiki.org Randy Shannon \\nRandy Shannon is a Beaver County , Pennsylvania Democratic Party activist. “A Democratic victory in 2016 with a bigger progressive caucus can tax Wall Street, end austerity and discrimination, and put the nation to work building the solar infrastructure we desperately need.” \\n“We need progressives like Sanders, who support working families, running for President, for Senate, and for Congress wherever possible,” said Randy Shannon , convener of the Sanders for President PA Exploratory Committee. \\nRandy Shannon was a student leader in the 1960’s at Duke University . He left Duke to organize campus groups for labor, peace, women’s equality and civil rights in the South as a staff member of the Southern Student Organizing Committee . In Nashville , he was a leader of the anti-Vietnam War movement and the Free Angela Davis campaign. He was an organizer for the National Welfare Rights Organization and led a local delegation to the 1972 Democratic Convention in Miami to fight for a $6400 guaranteed income. \\nHe ran in the TN 5th Congressional District Democratic primary in 1972 successfully targeting a right wing anti-busing candidate. He worked as a welder and organized rank and file workers as a member of Teamsters Local 327. He moved to Pittsburgh in 1976 and still works in the R&D sector of the basic materials industry. In PA he organized the Pittsburgh Youth Movement for Jobs , was active in the peace movement and progressive politics. \\nIn 1982, he moved to Beaver County and helped organize Beaver County Fightback , to defend the home ownership of unemployed steelworkers in the Ohio River Valley. He lead the Jesse Jackson campaign in the 4th and 22nd CDs of PA opening an office in Aliquippa in 1988. He also helped organize a ballot access campaign for Dennis Kucinich in 2004. He also helped organize the Beaver County Campaign for Nuclear Disarmament and is still active in Beaver County Peace Links . \\nUp until 1991, Randy Shannon was a member of the Communist Party USA . \\nShannon helped organize the first Citizens Congressional Hearing on Medicare for All, chaired by Rep. Dennis Kucinich in Aliquippa , PA in May 2005 and was an early advocate in Progressive Democrats of America for Medicare for All. He has worked for ten years building a local chapter of PDA that reflects the progressive coalition of minority, labor and progressive activists. For the last nine years he has been a member of the National Coordinating Committee of the Committees of Correspondence for Democracy and Socialism and a member of Democratic Socialists of America . \\nRandy Shannon attended the 6th National Convention of the Committees of Correspondence for Democracy and Socialism (CCDS) at San Francisco’s Whitcomb Hotel, July 23-26, 2009. \\nThe “Building the Progressive Majority: Race, Class and Gender” plenary discussion began a series of panel and workshop discussions. The plenary panel consisted of reports highlighting work of CCDS activists in the South, in the Heartland “rustbelt states,” on the West Coast and New England and the East Coast. Randy Shannon’s report on Western Pennsylvania and the dire conditions in the wake of de-industrialization was particularly moving. He described independent political work with groups like Progressive Democrats of America in raising the consciousness and unity of the working class and Black community, and then in turn ally with forces like the Congressional Progressive Caucus in the Congress to defeat the right and advance progressive planks in Obama’s economic package. He stressed the importance of ending the wars and healthcare reform, especially HR 676 “Medicare for All.”',\n",
       " 'Urban Population Booms Will Make Climate Change Worse Urban Population Booms Will Make Climate Change Worse Posted on Oct 27, 2016 \\nBy Tim Radford / Climate News Network Flooded slums in the densely-populated city of Jakarta, Indonesia. (Kent Clark via Flickr) \\nLONDON—The world’s cities are growing even faster than the human population. Within the last 40 years, the global population has increased by a factor of 1.8, but built-up areas have multiplied 2.5 times . All of this information, and much more, appears in a new European Commission (EC) publication called the Atlas of the Human Planet , prepared to coincide with the recent third UN Habitat conference in Quito, Ecuador. The Atlas shows that, 40 years ago, most of the world’s 4.1billion population lived in rural areas. Now more than half live in towns and cities—urban clusters that cover 7.6% of the planet’s land mass, equivalent to an area about half the size of the European Union. Most of the people in the world are crammed into urban centres with a density greater than 1,500 persons per square kilometre, and in settlements greater than 50,000 inhabitants. Altogether, geographers have identified 13,000 urban centres, altogether surrounded by 300,000 “urban clusters” of at least 5,000 inhabitants living at a density of 300 per square kilometre. Population tripled And in the 40 years since the first UN Habitat conference in 1976, the population of Africa has tripled, while the built-up area of the continent has quadrupled. In wealthy Europe, the population remained stable, but the built-up area doubled. The research for the Atlas has been enriched by a new, free, open and global dataset, the Global Human Settlement Layer , developed by the EC’s Joint Research Centre , and based on 12,400 billion individual satellite data readings over the past four decades. It provides, in every sense, an overview of a planet at work and at rest and struggling to survive. It confirms what most people would have suspected: that nine of the 10 most densely populated urban centres—including Cairo in Egypt, Guangzhou in China, and Jakarta in Indonesia—are in the low-income countries. Researchers warn that whatever problems these new city-dwellers have will be compounded by climate change The largest urban centre in the world is Los Angeles in the US, eight of the 10 largest urban centres are in the high-income countries, and five of those are in the US. And a group of scientists led by Timon McPhearson, assistant professor of urban ecology at the New School in New York , publish a warning in Nature journal that more urban areas will be built in the next 30 years than ever before just to house and shelter the additional 1.1billion people expected in the next 14 years—most of them in the crowded cities of Asia and Africa. Whatever problems these new city-dwellers have will be compounded, other researchers warn, by climate change —with ever more frequent and intense heatwaves, droughts, floods , and days of bad air quality. Around 40% of the world’s people live in coastal cities, and are therefore increasingly vulnerable to floods , tsunamis, surges and tropical storms. Because of the notorious urban “heat island effect”, cities are inevitably hotter than the surrounding countryside , and many are likely to face a crisis in the supply of safe, clean water . Swelling cities The new Atlas warns that the new, swelling cities will go on making ever greater demands on the farmland and wilderness beyond the city’s boundaries. In the last 15 years, 27,000 sq km of land was covered by housing, workshops and pavement. This is an area equal to Cyprus and Israel combined. If this growth continues at the present rate, an additional 1.1 million sq km of land will become built-up between 2015 and 2040—an area equal to the size of Ethiopia. In 119 countries, the urban population is between 70% and 90% of the total. In 25 countries—most of them in Asia—the city dwellers make up 90% of the population. Many of these are in megacities. The atlas records 50 “urban clusters” of more than 10 million people, and one gigacity—Beijing—that is home to more than 100 million. Inevitably, in this growth explosion, the poorest are often most at risk, from floods, landslides, and other geophysical and climate-related potential disasters. In the last 40 years, the number of people living at or even below sea level has almost doubled from 45 million to 88 million, and the number living on steep slopes has increased from 70 million to 160 million. \\nTim Radford, a founding editor of Climate News Network, worked for The Guardian for 32 years, for most of that time as science editor. He has been covering climate change since 1988. \\nAdvertisement',\n",
       " \" don't we have the receipt?\"]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "def tokenize_article(test_article):\n",
    "    test_article = test_article.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_article if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] if word in vocab_to_int else 0 for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized article\n",
    "test_ints = tokenize_article(test_data[1])\n",
    "# test_ints"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[[230,\n",
       "  9481,\n",
       "  1284,\n",
       "  2,\n",
       "  1670,\n",
       "  1250,\n",
       "  594,\n",
       "  1098,\n",
       "  230,\n",
       "  9481,\n",
       "  1284,\n",
       "  2,\n",
       "  1670,\n",
       "  1250,\n",
       "  594,\n",
       "  1098,\n",
       "  45047,\n",
       "  620,\n",
       "  620,\n",
       "  61690,\n",
       "  338,\n",
       "  2029,\n",
       "  3,\n",
       "  1,\n",
       "  230,\n",
       "  2029,\n",
       "  4086,\n",
       "  4954,\n",
       "  7863,\n",
       "  113,\n",
       "  1284,\n",
       "  2,\n",
       "  1670,\n",
       "  1250,\n",
       "  1594,\n",
       "  6,\n",
       "  1,\n",
       "  12741,\n",
       "  3,\n",
       "  1098,\n",
       "  1047,\n",
       "  19,\n",
       "  1,\n",
       "  230,\n",
       "  545,\n",
       "  1736,\n",
       "  16,\n",
       "  78609,\n",
       "  427,\n",
       "  6410,\n",
       "  2459,\n",
       "  2,\n",
       "  1193,\n",
       "  73,\n",
       "  1098,\n",
       "  24,\n",
       "  506,\n",
       "  18,\n",
       "  10227,\n",
       "  1,\n",
       "  620,\n",
       "  16,\n",
       "  1,\n",
       "  962,\n",
       "  2301,\n",
       "  3,\n",
       "  1,\n",
       "  2029,\n",
       "  4086,\n",
       "  6552,\n",
       "  200,\n",
       "  8,\n",
       "  2,\n",
       "  1670,\n",
       "  1612,\n",
       "  4,\n",
       "  540,\n",
       "  11322,\n",
       "  10,\n",
       "  1,\n",
       "  1250,\n",
       "  433,\n",
       "  1333,\n",
       "  8,\n",
       "  2,\n",
       "  2276,\n",
       "  1098,\n",
       "  63,\n",
       "  1,\n",
       "  508,\n",
       "  10,\n",
       "  1250,\n",
       "  1594,\n",
       "  48,\n",
       "  42,\n",
       "  20,\n",
       "  2,\n",
       "  2135,\n",
       "  40,\n",
       "  1,\n",
       "  195,\n",
       "  9,\n",
       "  6410,\n",
       "  24,\n",
       "  1,\n",
       "  506,\n",
       "  1,\n",
       "  620,\n",
       "  16,\n",
       "  1,\n",
       "  230,\n",
       "  200,\n",
       "  6,\n",
       "  1,\n",
       "  4841,\n",
       "  1099,\n",
       "  7910,\n",
       "  3,\n",
       "  1,\n",
       "  4954,\n",
       "  7863,\n",
       "  2029,\n",
       "  4086,\n",
       "  1,\n",
       "  1992,\n",
       "  502,\n",
       "  1612,\n",
       "  12358,\n",
       "  30454,\n",
       "  96366,\n",
       "  2170,\n",
       "  1,\n",
       "  267,\n",
       "  4,\n",
       "  476,\n",
       "  35235,\n",
       "  4018,\n",
       "  42997,\n",
       "  4,\n",
       "  54612,\n",
       "  57720,\n",
       "  182,\n",
       "  27,\n",
       "  1309,\n",
       "  443,\n",
       "  1672,\n",
       "  6,\n",
       "  283,\n",
       "  2,\n",
       "  3435,\n",
       "  1,\n",
       "  888,\n",
       "  106,\n",
       "  1250,\n",
       "  18,\n",
       "  7329,\n",
       "  14,\n",
       "  162,\n",
       "  14,\n",
       "  1,\n",
       "  7731,\n",
       "  7,\n",
       "  30,\n",
       "  191,\n",
       "  2,\n",
       "  445,\n",
       "  24,\n",
       "  48,\n",
       "  572,\n",
       "  2,\n",
       "  164,\n",
       "  1,\n",
       "  2482,\n",
       "  265,\n",
       "  2019,\n",
       "  3,\n",
       "  1,\n",
       "  2556,\n",
       "  5844,\n",
       "  4,\n",
       "  38828,\n",
       "  37,\n",
       "  410,\n",
       "  2,\n",
       "  2708,\n",
       "  9,\n",
       "  5,\n",
       "  55,\n",
       "  7759,\n",
       "  73,\n",
       "  1,\n",
       "  784,\n",
       "  181,\n",
       "  3,\n",
       "  1098,\n",
       "  1,\n",
       "  620,\n",
       "  275,\n",
       "  154,\n",
       "  2,\n",
       "  1,\n",
       "  620,\n",
       "  182,\n",
       "  42,\n",
       "  191,\n",
       "  55,\n",
       "  691,\n",
       "  139,\n",
       "  1,\n",
       "  4215,\n",
       "  508,\n",
       "  10,\n",
       "  1250,\n",
       "  13,\n",
       "  12,\n",
       "  16,\n",
       "  7,\n",
       "  1,\n",
       "  230,\n",
       "  9481,\n",
       "  6,\n",
       "  1,\n",
       "  4841,\n",
       "  1099,\n",
       "  42,\n",
       "  2474,\n",
       "  13758,\n",
       "  4787,\n",
       "  2196,\n",
       "  679,\n",
       "  13,\n",
       "  12,\n",
       "  23,\n",
       "  15864,\n",
       "  51,\n",
       "  4018,\n",
       "  44,\n",
       "  21,\n",
       "  1396,\n",
       "  9,\n",
       "  1,\n",
       "  9621,\n",
       "  7250,\n",
       "  230,\n",
       "  9481,\n",
       "  1113,\n",
       "  2,\n",
       "  283]]"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "# For each article, predict whether it's fake news (1) or not (0)\n",
    "def predict(net, test_article, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize article\n",
    "    test_ints = tokenize_article(test_article)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    return pred"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "preds = []\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs in test_data:\n",
    "    pred = predict(net, inputs)\n",
    "    preds.append(int(pred.item()))\n",
    "\n",
    "# -- stats! -- ##\n",
    "# print(\"Predictions: \", preds)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Predictions:  [0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finally, export the result to submit to the competition"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "import csv\n",
    "\n",
    "with open('submit.csv', 'w') as f:\n",
    "    f_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    f_writer.writerow(['id', 'label'])\n",
    "    for i in range(len(preds)):\n",
    "        f_writer.writerow([test_raw_data['id'][i], preds[i]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "submit_data = pd.read_csv('submit.csv', encoding='utf-8', header=0, keep_default_na=False)\n",
    "submit_data"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20801</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20804</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20806</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20807</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20808</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20809</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20810</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20811</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>20812</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20813</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20814</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20815</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>20816</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20818</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20819</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20821</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>20823</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>20824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>20825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20826</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>20827</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>20828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>20829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>25970</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5171</th>\n",
       "      <td>25971</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5172</th>\n",
       "      <td>25972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5173</th>\n",
       "      <td>25973</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5174</th>\n",
       "      <td>25974</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5175</th>\n",
       "      <td>25975</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5176</th>\n",
       "      <td>25976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5177</th>\n",
       "      <td>25977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>25978</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5179</th>\n",
       "      <td>25979</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5180</th>\n",
       "      <td>25980</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5181</th>\n",
       "      <td>25981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5182</th>\n",
       "      <td>25982</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5183</th>\n",
       "      <td>25983</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5184</th>\n",
       "      <td>25984</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5185</th>\n",
       "      <td>25985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5186</th>\n",
       "      <td>25986</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5187</th>\n",
       "      <td>25987</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5188</th>\n",
       "      <td>25988</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5189</th>\n",
       "      <td>25989</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5190</th>\n",
       "      <td>25990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5191</th>\n",
       "      <td>25991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>25992</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193</th>\n",
       "      <td>25993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>25994</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>25995</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>25996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>25997</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>25998</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5199</th>\n",
       "      <td>25999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  label\n",
       "0     20800      0\n",
       "1     20801      1\n",
       "2     20802      1\n",
       "3     20803      0\n",
       "4     20804      1\n",
       "5     20805      1\n",
       "6     20806      0\n",
       "7     20807      0\n",
       "8     20808      1\n",
       "9     20809      1\n",
       "10    20810      1\n",
       "11    20811      0\n",
       "12    20812      1\n",
       "13    20813      1\n",
       "14    20814      1\n",
       "15    20815      0\n",
       "16    20816      0\n",
       "17    20817      0\n",
       "18    20818      1\n",
       "19    20819      1\n",
       "20    20820      0\n",
       "21    20821      1\n",
       "22    20822      1\n",
       "23    20823      1\n",
       "24    20824      1\n",
       "25    20825      0\n",
       "26    20826      1\n",
       "27    20827      0\n",
       "28    20828      0\n",
       "29    20829      1\n",
       "...     ...    ...\n",
       "5170  25970      1\n",
       "5171  25971      0\n",
       "5172  25972      1\n",
       "5173  25973      0\n",
       "5174  25974      1\n",
       "5175  25975      0\n",
       "5176  25976      0\n",
       "5177  25977      0\n",
       "5178  25978      0\n",
       "5179  25979      0\n",
       "5180  25980      1\n",
       "5181  25981      1\n",
       "5182  25982      0\n",
       "5183  25983      0\n",
       "5184  25984      0\n",
       "5185  25985      0\n",
       "5186  25986      1\n",
       "5187  25987      0\n",
       "5188  25988      0\n",
       "5189  25989      1\n",
       "5190  25990      0\n",
       "5191  25991      0\n",
       "5192  25992      1\n",
       "5193  25993      1\n",
       "5194  25994      0\n",
       "5195  25995      0\n",
       "5196  25996      0\n",
       "5197  25997      1\n",
       "5198  25998      1\n",
       "5199  25999      1\n",
       "\n",
       "[5200 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}